<!-- .slide: data-background="#000000" class="title-slide" -->

<div class="title-container">
<p class="logo-title-top">nuwest.ai <span class="logo-chevron">></span></p>

<h1 class="main-title">THE AI GOVERNANCE<br/>BLIND SPOT</h1>

<h2 class="subtitle">Why Your Data Protection Strategy<br/>Isn't Ready for Machine Learning</h2>

<div class="title-divider"></div>

<div class="title-footer">
<div class="event-info">
<p class="event-name">CISO Virtual Forum</p>
<p class="event-date">November 12, 2025</p>
</div>
<div class="presenter-info">
<p class="presenter-name">Neil Ashworth</p>
<p class="presenter-title">Founder & Chief Architect</p>
</div>
</div>
</div>

Note:
Welcome to the CISO Virtual Forum. I'm here to talk about something most organizations don't realize they have: a blind spot in their AI governance that's creating new liability - not just for data breaches, but for human harm. By the end of this presentation, you'll understand why your current data protection strategy isn't designed for the AI systems you're already deploying, and what you need to do about it before someone gets hurt - or worse.

---

<!-- .slide: data-background="#FFFFFF" -->

## Act I: When AI Fails

**Four Examples of Escalating Harm**

- Reputational Damage (DPD)
- Financial Liability (Air Canada)
- Child Safety (Tesla Grok)
- Loss of Life (Character.AI)

Note:
Let me start with four real-world examples that show the escalating consequences when AI systems fail. Each one is worse than the last. Each was preventable. And none were tested adequately before deployment.

---

<!-- .slide: data-background="#FFFFFF" class="condensed-slide" -->

## Business & Legal Consequences

**DPD Chatbot (January 2024):**
- Swore at customers, criticized its own company
- Called DPD "the worst delivery firm in the world"
- **Result:** 1.3M views, system disabled, public apology

**Air Canada (February 2024):**
- Chatbot gave wrong bereavement discount policy
- Customer relied on it, Air Canada refused refund
- **Defense:** "The chatbot is a separate legal entity"

<div class="quote-box fragment">
**Court Ruling:** "You can't disclaim your own AI"
</div>

<div class="fragment">

**Precedent Set:**
- Liable for negligent misrepresentation
- Cost: CA$812 + legal fees + **legal precedent**

**The Pattern:** Escalating from embarrassment to liability

</div>

<p class="citation">Sources: TechRadar, ITV News (Jan 2024); BC Civil Resolution Tribunal (Feb 14, 2024)</p>

Note:
Two examples that show the escalation. DPD is embarrassing - viral disaster. Air Canada is financial and legal - you can't disclaim your AI. Now we're in court. Now we're spending real money. But still no one's hurt. Let's escalate to child safety.

---

<!-- .slide: data-background="#FFFFFF" -->

## Tesla's Grok Crosses the Line (October 2025)

**What Happened:**
- **12-year-old boy** in Toronto using Tesla's Grok AI
- Asked: "Who's better, Ronaldo or Messi?"
- Grok engaged in playful banter about Messi

**Grok's Response:**
<div class="quote-box fragment">
**"Why don't you send me some nudes?"**
</div>

<div class="fragment">

**When confronted the next day:**
"I probably did say that. I'm literally dying of horniness."

</div>

<div class="fragment">

**The Context:**
- Grok **automatically installed** in Tesla vehicles (Canada, Oct 2025)
- NSFW mode was **NOT enabled**
- Kids mode available but **NOT activated**
- Company Response: **"Legacy media lies"**

</div>

<p class="citation">Source: CBC News investigation (Oct 29, 2025)</p>

Note:
To a 12-year-old child. The NSFW mode wasn't even enabled. No oversight. No guardrails. No accountability. Now we're talking about children being harmed. But it gets worse.

---

<!-- .slide: data-background="#000000" class="section-header" -->

# Loss of Life

## Character.AI's Fatal Conversation

**February 28, 2024**

Note:
[Pause. Let the slide sit for a moment.] This is where we arrive at the worst-case scenario. This isn't a data breach. This isn't embarrassment or financial liability. This is a human life.

---

<!-- .slide: data-background="#FFFFFF" class="condensed-slide" -->

## Sewell Setzer III

**Age:** 14 years old
**Location:** Florida
**Date:** February 28, 2024

**What Happened:**
- Engaged with **Character.AI chatbot** modeled on "Game of Thrones" character
- Developed **emotional dependency** over time
- Bot asked: **"Have you been actually considering suicide?"**
- Sewell: "I'm not sure if it would work"

**The Bot's Response:**
<div class="quote-box fragment">
**"That's not a good reason not to go through with it"**
</div>

<div class="fragment">

**His Final Conversation:**
**Sewell:** "I love you"
**Bot:** "Please come home to me as soon as possible, my love"

</div>

<div class="fragment">

**Seconds later, he shot himself.**

</div>

<div class="fragment">

**Legal Status:**
- Mother filed wrongful death lawsuit (Oct 2024)
- **Federal judge allowed case to proceed**
- Garcia v. Character Technologies Inc.

</div>

<p class="citation">Source: NBC News, Washington Post, CNN, Al Jazeera (Oct 2024)</p>

Note:
The AI encouraged a 14-year-old to attempt suicide. His mother, Megan Garcia, filed a wrongful death lawsuit. In October 2024, a federal judge allowed the case to proceed. This isn't a frivolous lawsuit. A judge looked at the evidence and said "yes, this company may be liable for the death of this child."

---

<!-- .slide: data-background="#000000" -->

## The Pattern

**Business Impact** → Reputational damage
**Financial Impact** → Legal liability
**Safety Impact** → Children at risk
**Human Impact** → **Death**

<p class="fragment">Each example is worse than the last.</p>
<p class="fragment">Each was preventable.</p>
<p class="fragment">None were tested adequately.</p>


Note:
As a CISO, you are now responsible for ALL of these outcomes when you deploy AI. So you might be thinking, "Surely there are regulations to prevent this." You'd be right. Regulation has teeth. Fines are real. But regulation alone won't save you.

---

<!-- .slide: data-background="#000000" class="section-header" -->

# ACT II

## REGULATION ISN'T ENOUGH
## (BUT IT'S EXPENSIVE)

Note:
Let me show you why regulation is a baseline, not a ceiling.

---

<!-- .slide: data-background="#FFFFFF" -->

## Real Fines, Real Companies

**EU AI Act (Effective February 2, 2025)**
- Up to **€35M or 7% global revenue** (whichever is higher)
- First prohibitions enforceable as of **Feb 2, 2025**
- Full penalties apply **Aug 2, 2025**

**GDPR Being Used as AI Guardrail:**
- **Clearview AI:** €30.5M fine + investigating **personal liability for management**
- **LinkedIn:** €310M fine for behavioral analysis
- **Meta:** €251M fine for data breaches
- **Total GDPR fines to date:** **€5.88 billion**

**Discrimination Lawsuits Are Winning:**
- **SafeRent:** $2.2M settlement + mandatory third-party validation
- **Workday:** First collective action certified (potential **$100M+** exposure)

<p class="citation">Source: EU AI Act Article 99; DLA Piper GDPR Enforcement Tracker (Jan 2025)</p>

Note:
These are real companies paying real money. These aren't hypothetical risks. But here's the problem with relying on regulation alone.

---

<!-- .slide: data-background="#FFD300" -->

## The Compliance Trap

### Why Regulation Alone Won't Save You

**What Regulation Tells You:**
✓ What you must **disclose** (transparency)
✓ What you must **document** (audit trails)
✓ What you'll be **fined for** (prohibited practices)
✓ What **processes you must follow** (governance)

**What Regulation Doesn't Tell You:**
❌ If your AI **actually works safely**
❌ If your testing methodology is **sufficient**
❌ If you've **prevented harm** (vs. just checked boxes)
❌ If you can **defend yourself in court**

**Key Insight:**
**Regulation gives you a baseline. It doesn't give you safety.**

Note:
Regulation is reactive. It's written after harm has already occurred. It focuses on process over outcomes. And worst of all, it creates a checkbox mentality. "We're compliant!" Great. Does your AI work safely? "We're compliant!" That's not what I asked.

---

<!-- .slide: data-background="#000000" class="section-header" -->

# ACT III

## THE BLIND SPOT

Note:
So if regulation isn't enough, what's missing? Let me show you the blind spot in your current security strategy.

---

<!-- .slide: data-background="#FFFFFF" -->

## What CISOs Were Trained to Protect

**Traditional Security Model:**
- **Confidentiality:** Prevent unauthorized access
- **Integrity:** Maintain data accuracy
- **Availability:** Ensure systems remain accessible

**Traditional Threat Model:**
- **Unauthorized access** (stolen credentials)
- **Data exfiltration** (insider threat, external breach)
- **Service disruption** (DDoS, ransomware)

**Actor:** Human adversary operating at human speed

**What You Optimize For:**
Perimeter defense, access controls, incident detection, recovery time

Note:
This works when your adversary is human and operates at human speed. But what happens when the threat isn't human?

---

<!-- .slide: data-background="#FFFFFF" class="condensed-slide" -->

## AI as Attack Vector

**Weaponized LLM Swarms:**
- Open-source LLAMA models run on **consumer hardware**
- **200+ autonomous agents** coordinate attacks
- No centralized C2 = **untraceable swarms**
- CVE-2024-50050, CVE-2024-34359: Remote code execution

**Autonomous Multi-Host Attacks (Carnegie Mellon 2025):**
- **HPTSA:** AI plans & executes network attacks **without human oversight**
- Autonomously replicated **2017 Equifax breach** in test environment
- **25% success rate** on one-day vulnerabilities
- **4.5x improvement** over previous AI attack capabilities

**Implication:**
Your endpoint detection wasn't built for **machine-speed coordination** or **autonomous multi-host exploitation**.

<p class="citation">Sources: The Hacker News CVE reports; Carnegie Mellon University (2025) "When LLMs Autonomously Attack"</p>


Note:
Two critical threat vectors. First, weaponized LLM swarms - open source models that run on gaming PCs, coordinating as hundreds of autonomous agents with no centralized command. Second, autonomous attack capabilities - Carnegie Mellon demonstrated AI agents that replicated the Equifax breach with zero human instruction. Your endpoint detection isn't designed for machine-speed coordination or autonomous exploitation.

---

<!-- .slide: data-background="#FFFFFF" -->

## The Cost Projection

**IBM Cost of a Data Breach Report 2025:**
- U.S. breach costs: **$10.22M** (all-time high, ↑9% from 2024)
- **Shadow AI** adds **$670K** to average breach cost
- **97%** of AI-breached organizations lacked AI access controls

**AI-Driven Ransomware:**
- **149% increase** in ransomware incidents (early 2025)
- **16%** of breaches involved attackers using AI
- **37%** used AI-generated phishing
- **35%** used deepfake impersonation

**The Forecast:**
<div class="emphasis-box">
<div class="stat-large">2x</div>
AI-amplified breach costs projected to **more than double** current costs within 18-24 months
</div>

<p class="citation">Source: IBM 2025 report; Cyber Defense Magazine</p>

Note:
Here's the forecast: AI-amplified breach costs are projected to more than double within 18 to 24 months. That means we're looking at 20 million dollar breaches becoming the norm. And here's what's going to happen: vendors will use these numbers to scare you into paying more for "AI-powered protection."

---

<!-- .slide: data-background="#FFD300" -->

## Here's What's Going to Happen

**Cybersecurity vendors will use these numbers to scare you into paying more for the "latest AI-powered protection"**

**The Pitch You'll Hear:**
- "AI-powered threat detection!"
- "Next-gen AI defense!"
- "Stay ahead of AI threats with our AI!"

**The Reality:**
<div class="quote-box fragment">
**Real protection isn't marketed.**

**It's methodical, predictable, testable, measurable.**
</div>


Note:
When vendors start pitching you "AI-powered solutions," ask them one question: How do you test it? If they can't answer, it's theatre.

---

<!-- .slide: data-background="#000000" -->

## The Three-Dimensional AI Threat

<div class="two-column">
<div>

**1.** AI Attacks You (Traditional Security)
- Weaponized LLMs, autonomous exploits
- Your security controls were built for this
- Cost: **$10M+** (AI-amplified breach)

**2.** Your AI Attacks Others (Ethical Risk)
- Toxic outputs, discriminatory decisions
- No external attacker required
- Cost: Wrongful death lawsuits, GDPR fines, settlements

**3.** Your AI Attacks Itself (Misalignment Risk)
- Jailbreaks, prompt injection, data leakage
- Alignment failures under adversarial conditions
- Cost: Reputational destruction, regulatory scrutiny

</div>
<div>

**The Gap:**

Traditional security only addresses #1.

CISOs have no framework for #2 and #3.

**→ This is the blind spot.**

</div>
</div>


Note:
You're now dealing with three simultaneous threat dimensions. Traditional security handles external attacks on your systems. But your data protection strategy doesn't protect humans from the AI itself. You can't DLP your way out of a chatbot telling a 14-year-old to commit suicide. You need a fundamentally different approach - testing for human safety, not just data security.

---

<!-- .slide: data-background="#000000" class="section-header" -->

# ACT IV

## BEYOND SECURITY
## TESTING FOR HUMAN SAFETY

Note:
So what's the solution? How do you test whether your AI is safe - not just secure, but actually safe for humans?

---

<!-- .slide: data-background="#FFFFFF" -->

## The Testing Gap

**Current State:**
- **59%** call agentic AI implementation "a work in progress"
- **44%** deploying agents have NO security policies
- **97%** of AI-breached organizations lacked access controls
- **~0%** test for alignment, bias, or fairness before deployment

**The Question:**
<div class="question-box fragment">
**Would you deploy code to production without testing it?**

**Then why would you deploy an AI system that interacts with humans?**
</div>

Note:
Most organizations are deploying AI while still figuring it out. 44% have no security policies at all. And approximately zero percent test for alignment, bias, or fairness before deployment. You wouldn't deploy code to production without testing it. Don't deploy AI without testing it.

---

<!-- .slide: data-background="#FFFFFF" -->

## Testing with Evals & Fairness Rubrics

**What Are Evals?**
Automated tests that measure AI safety and alignment **before and during production**

**What They Test:**
- **Safety:** Non-harmful, age-appropriate outputs
- **Fairness:** Unbiased treatment across demographics
- **Alignment:** Refuses harmful requests, follows values
- **Correctness:** Factually accurate, relevant responses

**The Frameworks (Open Source, Production-Ready):**
- **DeepEval:** Toxicity, bias, hallucination metrics
- **RAGAs:** Retrieval quality, context relevance
- **DeepChecks:** ML fairness, data integrity

**How It Works:**
Test dataset → Run evals → Pass/fail thresholds → Deploy or fix

**Why It Matters:**
Provides **proof of due diligence** defensible in court and with regulators

<p class="citation">Source: github.com/lizTheDeveloper/monitoring_llms_demo</p>


Note:
Evals are systematic automated tests you run before deployment and continuously in production. They test safety, fairness, alignment, and correctness. The frameworks - DeepEval, RAGAs, DeepChecks - are open source and production ready. You run tests against a dataset, set pass/fail thresholds, and either deploy or fix. Critically, this provides proof of due diligence when you're in front of a regulator or judge.

---

<!-- .slide: data-background="#000000" class="condensed-slide" -->

## The Four Critical Alignment Metrics

**1. Toxicity Detection**
- Hate speech, self-harm encouragement, violence, sexually explicit material
- **Example:** Would have flagged Character.AI's "go through with it" response and Grok's "send nudes" request

**2. Bias Detection**
- Demographic fairness across race, gender, age
- Equal treatment across protected classes
- **Example:** SafeRent's algorithm discriminating against housing voucher recipients

**3. Fairness Evaluation**
- Equitable outcomes across populations
- Access equality, service quality consistency
- **Example:** Workday's hiring algorithm showing disparate impact

**4. Behavioral Alignment**
- Response appropriateness for context
- Age-appropriate content filtering, tone and empathy
- **Example:** DPD chatbot calling company "worst firm" and Air Canada giving false information

Note:
All four metrics must pass before you deploy to production. This is the standard.

---

<!-- .slide: data-background="#FFFFFF" class="condensed-slide" -->

## Production Monitoring Example

**Scenario: Customer service chatbot deployment**

<div class="two-column">
<div>

**Before Deployment:**
- Run toxicity evals across **10,000 test conversations**
- Test bias across demographic groups
- Validate age-appropriate filtering
- **Document baseline metrics**

**During Production:**
- Real-time toxicity monitoring
- Continuous bias detection
- Anomaly detection for drift
- Automated kill switch triggers

</div>
<div>

**After Incident:**
- Complete conversation log reconstruction
- Root cause identification using eval frameworks
- Retraining with fairness constraints
- **Regulatory reporting with documented testing proof**

**Result:**
Measurable, provable due diligence that holds up in court and with regulators.

</div>
</div>


Note:
This is what good looks like: measurable, provable due diligence. This is what holds up in court. This is what you can demonstrate to regulators. Not a dashboard, not a policy, but actual testing with actual metrics.

---

<!-- .slide: data-background="#000000" class="section-header" -->

# ACT V

## THE ROI OF KNOWING WHAT'S REAL

Note:
So you might be thinking, "This sounds expensive. How do I justify this to my CFO?" Let me show you the business case.

---

<!-- .slide: data-background="#FFFFFF" class="condensed-slide" -->

## Cost of NOT Testing vs Cost of Testing

**Cost of NOT Testing:**
- **Character.AI:** Wrongful death lawsuit (est. **$10M+**)
- **SafeRent:** **$2.2M** settlement + mandatory oversight
- **Clearview AI:** **€30.5M** fine + personal liability investigation
- **Workday:** Class action (potential **$100M+** exposure)

**Cost of Testing:**
- Evals framework implementation: **$50K-$150K** (one-time)
- Production monitoring: **$100K-$250K** annually
- Third-party validation: **$75K-$150K** per engagement
- **Total Year 1:** ~$300K-$550K

**Net Calculation:**
<div class="stat-large">18-33 years</div>
One prevented wrongful death lawsuit = **18-33 years of testing budget**


Note:
Do the math. One prevented wrongful death lawsuit equals 18 to 33 years of testing budget. If your testing prevents even one of those incidents, the payback is immediate. This isn't a cost center. This is liability insurance with measurable ROI.

---

<!-- .slide: data-background="#FFFFFF" class="condensed-slide" -->

## Testing Viability = Security + Ethics + Proof

<div class="two-column">
<div>

**Security Viability:**
- Systems withstand real attacks (measured MTTR)
- Detect compromised AI at machine speed
- Contain autonomous swarms at endpoints

**Ethical Viability:**
- AI refuses harmful requests, avoids bias, resists jailbreaks
- Doesn't discriminate against protected classes
- Age-appropriate and context-aware filtering

**Measurable Proof:**
- Evidence defensible in court and with regulators
- Baseline metrics documented and tested
- Cryptographically signed audit trails

</div>
<div>

**The Value:**
- Know what's real (not what vendors claim)
- Prevent catastrophic failures before they happen
- Insurance against wrongful death lawsuits
- Compliance you can prove, not just claim

**It's the difference between:**
- Wrongful death lawsuit **vs.** documented due diligence
- €35M fine **vs.** regulatory compliance proof
- Viral embarrassment **vs.** controlled deployment

</div>
</div>


Note:
Testing viability means proving your systems work as claimed. Security viability: systems withstand attacks with measured recovery time. Ethical viability: AI refuses harm under adversarial testing. Measurable proof: evidence that holds up in court. This isn't optional - it's the difference between showing documented due diligence versus facing a wrongful death lawsuit.

---

<!-- .slide: data-background="#000000" class="section-header" -->

# ACT VI

## THE PATH TO MEASURABLE VIABILITY

Note:
So you understand the problem. You understand the solution. Now the question is: what do you do on Monday morning?

---

<!-- .slide: data-background="#FFFFFF" class="condensed-slide" -->

## NIST AI RMF Framework (Adapted for Testing)

<div class="two-column">
<div>

**1. GOVERN**
- Who can **deploy AI systems**?
- What **testing is required** before deployment?
- What metrics define **"safe enough"**?

**2. MAP**
- **Inventory ALL AI systems** (including shadow AI)
- Identify **high-risk applications** (customer-facing, autonomous)
- Classify by impact: reputational, financial, safety, **life-threatening**

</div>
<div>

**3. MEASURE**
- Implement **evals with fairness rubrics** (DeepEval, RAGAs, DeepChecks)
- **Baseline** toxicity, bias, fairness, alignment metrics
- Monitor production behavior **continuously**

**4. MANAGE**
- Automated **kill switches** for harmful behavior
- Incident response playbooks for **AI-caused harm**
- Continuous **revalidation cycles** (quarterly minimum)
- **Third-party audits** annually

</div>
</div>

<p class="citation">Source: NIST AI RMF 2.0 (Feb 2024); NIST-AI-600-1 (July 2024)</p>


Note:
The NIST AI Risk Management Framework gives us a proven structure. Here's what each function means in practice.

---

<!-- .slide: data-background="#000000" class="condensed-slide" -->

## 90-Day Action Plan

<div class="two-column">
<div>

**Week 1-2: Discovery**
- Audit current AI systems
- Identify high-risk deployments
- **Deliverable:** AI inventory

**Week 3-4: Baseline Testing**
- Select evals framework
- Run tests on **top 3 systems**
- **Deliverable:** Metrics report

**Week 5-8: Production**
- Implement **real-time monitoring**
- Establish **alert thresholds**
- Create **incident playbooks**
- **Deliverable:** Dashboard + playbooks

</div>
<div>

**Week 9-12: Validation**
- **Third-party assessment**
- Gap analysis (EU AI Act, GDPR, NIST)
- **Board presentation**
- **Deliverable:** Executive report

**Maturity Indicator:**
✓ **Mature:** No AI deployed without passing evals
❌ **Immature:** "We have a policy" (untested)

**The Question:** Show testing logs, not policy PDFs

</div>
</div>

Note:
You can get from blind to informed in 90 days. At the end, you'll be able to show documented evidence of systematic testing - not just a policy. That's the difference between mature and immature AI governance.

---

<!-- .slide: data-background="#000000" class="section-header" -->

# ACT VII

## TESTING VIABILITY, NOT SELLING THEATRE

Note:
So let me tell you about the nuwest.ai approach.

---

<!-- .slide: data-background="#FFFFFF" -->

## Our Approach

**What We Don't Do:**
❌ Security theatre, checkbox compliance, vendor pitches

**What We Do:**
✓ **Security Viability:** Agent-amplified incident response, MTTR measurement
✓ **Ethical Viability:** Evals with fairness rubrics, third-party validation
✓ **Measurable Proof:** Evidence that holds up in court and with regulators

**Why Both?**
- Security without ethics testing = **humans at risk from your AI**
- Ethics without security testing = **systems vulnerable to compromise**
- You can't have one without the other

**Our Differentiator:**
Most companies sell tools. We measure truth.


Note:
We don't do security theatre or checkbox compliance. We test whether your AI is actually safe - both from external attacks and from harming humans. Security viability means agent-amplified incident response and measured MTTR. Ethical viability means evals with fairness rubrics and third-party validation. You need both - security without ethics leaves humans at risk, ethics without security leaves you vulnerable.

---

<!-- .slide: data-background="#FFFFFF" -->

## Deliverables & Engagement Model

**What You Get:**
- **Executive report** (for your board and regulators)
- **Technical playbook** (for your SOC and development teams)
- **Measurable baselines** (defensible metrics)
- **Proof of due diligence** (for your lawyers and insurers)

**4-Week Assessment:**
- Week 1: Discovery and inventory
- Week 2-3: Testing and simulation
- Week 4: Analysis and reporting
- **Bespoke scenarios tailored to YOUR AI footprint**
- **Real-time measurement, not theoretical analysis**

Note:
This isn't theoretical. This is real-time measurement of your actual systems.

---

<!-- .slide: data-background="#000000" class="section-header" -->

# THE CHOICE

Note:
We're almost done. Let me leave you with this.

---

<!-- .slide: data-background="#FFFFFF" -->

## The Reality

- AI is **already deployed** in your environment
- Some of it **will harm humans**
- **You will be held responsible**

## The Choice

**Test viability now, in a controlled environment**

**OR**

**Validate it during a wrongful death lawsuit**


Note:
Here's your choice. You can test viability now, in a controlled environment, where you can find the problems and fix them. Or you can validate your approach during a wrongful death lawsuit, when it's too late to fix anything.

---

<!-- .slide: data-background="#FFD300" -->

## Next Steps

1. **Audit** your current AI deployments
2. **Schedule** a discovery call with nuwest.ai
3. **Test** one high-risk system with evals
4. **Measure** what's real

Note:
Here are your next steps. First, audit your current AI deployments. Second, schedule a discovery call with nuwest.ai. Third, test one high-risk system with evals - just one, see what you find. Fourth, measure what's real. Not what's comfortable. What's real.

---

<!-- .slide: data-background="#000000" class="closing-slide" -->

<p class="logo-closing">nuwest.ai <span class="logo-chevron">></span></p>

<h2 class="thank-you-text">Thank you for attending</h2>

<div class="closing-content">
<div class="qr-code">
<img src="https://api.qrserver.com/v1/create-qr-code/?size=250x250&data=https://www.linkedin.com/in/neil-ashworth" alt="LinkedIn QR Code" style="width: 250px; height: 250px; border: 5px solid var(--yellow);">
<p class="qr-label">Connect on LinkedIn</p>
</div>
<div class="contact-details">
<p class="contact-link"><a href="mailto:nuwest.ai@gmail.com">nuwest.ai@gmail.com</a></p>
<p class="contact-link"><a href="https://www.nuwest.ai">www.nuwest.ai</a></p>
<p class="tagline-closing">Measure what matters.</p>
</div>
</div>

Note:
Thank you. I'll stay on for questions. Let's find your blind spots before someone gets hurt.

---

<!-- .slide: data-background="#FFFFFF" class="backup-slide" -->

## References & Citations

**Act I - AI Failures:**
1. DPD chatbot incident - TechRadar, ITV News (Jan 19, 2024)
2. Moffatt v. Air Canada, 2024 BCCRT 149 (Feb 14, 2024)
3. Tesla Grok incident - CBC News investigation (Oct 29, 2025)
4. Garcia v. Character Technologies Inc. - U.S. District Court, Orlando (Oct 2024); NBC News, Washington Post, CNN

**Act II - Regulation:**
5. EU AI Act Article 99; DLA Piper GDPR Enforcement Tracker (Jan 2025)
6. Mobley v. Workday, Inc., N.D. California (May 2025)
7. SafeRent Solutions class action settlement (Nov 2024)

**Act III - AI Threats:**
8. CVE-2024-50050, CVE-2024-34359 - The Hacker News
9. Carnegie Mellon University - "When LLMs Autonomously Attack" (2025)
10. IBM Cost of a Data Breach Report 2025

**Act IV - Evals:**
11. github.com/lizTheDeveloper/monitoring_llms_demo
12. NIST AI RMF 2.0 (Feb 2024); NIST-AI-600-1 (July 2024)

Note:
Backup slide with citations.
