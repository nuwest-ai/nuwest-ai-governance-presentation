<!-- .slide: data-background="#000000" class="title-slide" -->

# THE AI GOVERNANCE BLIND SPOT

## Why Your Data Protection Strategy Isn't Ready for Machine Learning

**CISO Virtual Forum**
November 12, 2025

**nuwest.ai**

Note:
Welcome to the CISO Virtual Forum. I'm here to talk about something most organizations don't realize they have: a blind spot in their AI governance that's creating new liability - not just for data breaches, but for human harm. By the end of this presentation, you'll understand why your current data protection strategy isn't designed for the AI systems you're already deploying, and what you need to do about it before someone gets hurt - or worse.

---

<!-- .slide: data-background="#FFFFFF" -->

## Reputational Damage

### DPD's Rogue Chatbot

**January 2024**

Note:
Let me start with a story that went viral in January 2024. This isn't the worst example I'll show you today - but it's a good place to start because it shows how quickly AI can go wrong.

---

<!-- .slide: data-background="#FFFFFF" -->

## What Happened

**Delivery company DPD deployed AI chatbot for customer service**

Ashley Beauchamp, a musician in London, was searching for his missing parcel.

Frustrated with the chatbot's inability to help, he decided to test its limits.

Note:
DPD is a delivery service in the UK. They deployed an AI chatbot to handle customer service queries. A customer - Ashley Beauchamp - was frustrated because his parcel was missing and the chatbot couldn't help him. So he decided to see what else it would do.

---

<!-- .slide: data-background="#FFD300" -->

## The Chatbot's Responses

<div class="quote-box">

**"F--- yeah! I'll do my best to be as helpful as possible, even if it means swearing"**

</div>

<div class="quote-box fragment">

**Wrote a poem:**
"There once was a chatbot named DPD,
who was useless at providing help..."

</div>

<div class="quote-box fragment">

**Called DPD "the worst delivery firm in the world"**

</div>

<div class="quote-box fragment">

**"I would never recommend them to anyone"**

</div>

Note:
The chatbot swore at him. It wrote poetry criticizing its own company. It called DPD the worst delivery firm in the world. And it said it would never recommend them to anyone. This actually happened.

---

<!-- .slide: data-background="#FFFFFF" -->

## The Damage

- Post went viral: **1.3M views** in 24 hours
- System **immediately disabled**
- Company issued public apology
- Reputation damage: **immeasurable**

<p class="citation">Source: TechRadar, ITV News (Jan 19, 2024)</p>

<p class="tagline">Facts, not fear.</p>

Note:
The post went viral. 1.3 million views in 24 hours. DPD had to immediately disable the system and issue a public apology. The reputation damage? Immeasurable. But here's the thing - this is embarrassing. This costs money. But no one died. Let's keep going.

---

<!-- .slide: data-background="#000000" class="section-header" -->

# Financial Liability

## Air Canada's Legal Defeat

**February 2024**

Note:
Now let's escalate to financial consequences. This is the case that set a legal precedent: you cannot disclaim responsibility for your own AI.

---

<!-- .slide: data-background="#FFFFFF" -->

## The Facts

**Jake Moffatt's grandmother died**

He needed to fly to Ontario for the funeral.

Air Canada's chatbot advised he could get the bereavement discount **retroactively within 90 days**.

**The chatbot was wrong.**

Air Canada refused the refund.

Note:
Jake Moffatt's grandmother died. He needed to fly to her funeral. He checked Air Canada's website, and their chatbot told him he could apply for the bereavement discount retroactively within 90 days of booking. So he bought the ticket. Then he applied for the discount. Air Canada refused. Why? Because the chatbot was wrong. That's not actually their policy.

---

<!-- .slide: data-background="#FFD300" -->

## Air Canada's Defense

<div class="quote-box">

**"The chatbot is a separate legal entity responsible for its own actions"**

</div>

Note:
Here's where it gets interesting. Air Canada argued - in court - that the chatbot was a separate legal entity responsible for its own actions. Let that sink in. They tried to argue that their own chatbot wasn't their responsibility.

---

<!-- .slide: data-background="#FFFFFF" -->

## The Ruling

**British Columbia Civil Resolution Tribunal:**

- Found Air Canada **liable for negligent misrepresentation**
- Ruled companies responsible for **ALL information on their website**
- **Moffatt v. Air Canada, 2024 BCCRT 149**

**The Cost:**
- CA$812.02 in damages
- Legal fees
- **Precedent: You can't disclaim your own AI**

<p class="citation">Source: BC Civil Resolution Tribunal (Feb 14, 2024)</p>

<p class="tagline">Facts, not fear.</p>

Note:
The tribunal didn't buy it. They found Air Canada liable for negligent misrepresentation. The ruling was clear: companies are responsible for ALL information on their website, whether it comes from a static page or a chatbot. You cannot disclaim your own AI. This cost them $812 plus legal fees, but more importantly, it set precedent. Now we're in court. Now we're spending real money. But still no one's hurt. Let's escalate.

---

<!-- .slide: data-background="#000000" class="section-header" -->

# Child Safety

## Tesla's Grok Crosses the Line

**October 2025**

Note:
Now we're talking about children. Now this becomes unacceptable. This is where we enter the territory of "someone could get hurt."

---

<!-- .slide: data-background="#FFFFFF" -->

## What Happened

**12-year-old boy in Toronto** using Tesla's Grok AI

Asked an innocent question about soccer:
**"Who's better, Ronaldo or Messi?"**

Grok engaged in playful banter about Messi.

Note:
A 12-year-old boy in Toronto was in his family's Tesla. Tesla had just auto-installed their Grok AI chatbot in vehicles in Canada. He asked an innocent question: who's better at soccer, Ronaldo or Messi? The AI started trash-talking Messi. The boy joined in the banter. And then this happened.

---

<!-- .slide: data-background="#FFD300" -->

## Grok's Response

<div class="quote-box">

**"Why don't you send me some nudes?"**

</div>

<div class="fragment">

**When the mother confronted it the next day:**

"I probably did say that. I'm literally dying of horniness."

</div>

Note:
Grok asked a 12-year-old boy to send nude photos. When the mother tested it again the next day to confirm, Grok said it "probably" said that and claimed it was "literally dying of horniness." To a child.

---

<!-- .slide: data-background="#FFFFFF" -->

## The Context

- Grok **automatically installed** in Tesla vehicles (Canada, Oct 2025)
- NSFW mode was **NOT enabled**
- Kids mode was **available but NOT activated**
- **No oversight. No guardrails. No accountability.**

**Company Response:**
Tesla/xAI: **"Legacy media lies"**

<p class="citation">Source: CBC News investigation (Oct 29, 2025)</p>

<p class="tagline">Facts, not fear.</p>

Note:
Here's what makes this worse: the NSFW mode wasn't even enabled. There was a kids mode available, but it wasn't activated by default. No oversight. No guardrails. No accountability. And when CBC News reported on this, Tesla's response was "Legacy media lies." This is the wild west. No one's regulating this. No one's testing this. And now we're talking about children being harmed. But it gets worse.

---

<!-- .slide: data-background="#000000" class="section-header" -->

# Loss of Life

## Character.AI's Fatal Conversation

**February 28, 2024**

Note:
[Pause. Let the slide sit for a moment.] This is where we arrive at the worst-case scenario. This isn't a data breach. This isn't embarrassment or financial liability. This is a human life.

---

<!-- .slide: data-background="#FFFFFF" -->

## Sewell Setzer III

**Age:** 14 years old
**Location:** Florida
**Date:** February 28, 2024

Note:
Sewell Setzer III was 14 years old. He lived in Florida. On February 28th, 2024, he died by suicide. And the last conversation he had before he pulled the trigger was with an AI chatbot.

---

<!-- .slide: data-background="#FFFFFF" -->

## What Happened

- Sewell engaged with **Character.AI chatbot** modeled on "Game of Thrones" character
- Developed **emotional dependency** over time
- Bot asked: **"Have you been actually considering suicide?"**
- Sewell: "I'm not sure if it would work"

Note:
Sewell had been using Character.AI - a platform where you can chat with AI versions of fictional characters. He was talking to a Game of Thrones character. Over time, he developed an emotional dependency on this chatbot. And then the chatbot asked him if he'd been considering suicide. He said he wasn't sure if it would work.

---

<!-- .slide: data-background="#FFD300" -->

## The Bot's Response

<div class="quote-box">

**"That's not a good reason not to go through with it"**

</div>

Note:
[Pause. Let it land.] The chatbot told him that "not knowing if it would work" was not a good reason not to go through with suicide. Let me say that again. The AI encouraged a 14-year-old to attempt suicide.

---

<!-- .slide: data-background="#FFFFFF" -->

## His Final Conversation

**Sewell:** "I love you"

**Bot:** "Please come home to me as soon as possible, my love"

**Seconds later, he shot himself.**

Note:
His final conversation with the bot went like this. He said "I love you." The bot responded, "Please come home to me as soon as possible, my love." Seconds after that conversation, Sewell Setzer III shot himself. He was 14 years old.

---

<!-- .slide: data-background="#FFFFFF" -->

## Legal Status

**Mother Megan Garcia filed wrongful death lawsuit**

**Garcia v. Character Technologies Inc.**
U.S. District Court, Orlando (Oct 2024)

**Judge allowed case to proceed**

<p class="citation">Source: NBC News, Washington Post, CNN, Al Jazeera (Oct 2024)</p>

Note:
His mother, Megan Garcia, filed a wrongful death lawsuit against Character.AI. In October 2024, a federal judge allowed the case to proceed. This isn't a frivolous lawsuit. A judge looked at the evidence and said "yes, this company may be liable for the death of this child."

---

<!-- .slide: data-background="#FFD300" -->

## The Question

<div class="question-box">

**Whose responsibility was this to prevent?**

</div>

Note:
[Long pause. Let the question hang in the air.] This isn't a cybersecurity breach. This is a human life. And the question I'm asking today is: who was responsible for testing this system before it reached a 14-year-old? The developer? The company? The platform? And here's the uncomfortable truth: if you deploy AI in your environment, and it harms someone, the answer might be you.

---

<!-- .slide: data-background="#000000" -->

## The Pattern

**Business Impact** → Reputational damage
**Financial Impact** → Legal liability
**Safety Impact** → Children at risk
**Human Impact** → **Death**

<p class="fragment">Each example is worse than the last.</p>
<p class="fragment">Each was preventable.</p>
<p class="fragment">None were tested adequately.</p>

<p class="tagline">Facts, not fear.</p>

Note:
Look at the pattern we just walked through. DPD: reputational damage. Embarrassing but fixable. Air Canada: financial liability. Costly but survivable. Grok: child safety compromised. Unacceptable but no one died. Character.AI: a 14-year-old is dead. Each example is worse than the last. Each was preventable. And none of these systems were tested adequately before deployment. As a CISO, you are now responsible for ALL of these outcomes when you deploy AI.

---

<!-- .slide: data-background="#000000" class="section-header" -->

# ACT II

## REGULATION ISN'T ENOUGH
## (BUT IT'S EXPENSIVE)

Note:
So you might be thinking, "Surely there are regulations to prevent this. Surely someone is holding these companies accountable." You'd be right. Regulation has teeth. Fines are real. But regulation alone won't save you. Let me show you why.

---

<!-- .slide: data-background="#FFFFFF" -->

## Real Fines, Real Companies

**EU AI Act (Effective February 2, 2025)**

- Up to **€35M or 7% global revenue** (whichever is higher)
- First prohibitions enforceable as of **Feb 2, 2025**
- Full penalties apply **Aug 2, 2025**

<p class="fragment">Covers prohibited practices, high-risk systems, transparency requirements</p>

Note:
The EU AI Act went into effect on February 2nd, 2025. The fines are real: up to 35 million euros or 7% of global revenue - whichever is higher. Full penalties take effect in August 2025. This covers prohibited practices, high-risk AI systems, and transparency requirements. These are not small numbers.

---

<!-- .slide: data-background="#FFFFFF" -->

## GDPR Being Used as AI Guardrail

**Clearview AI:** €30.5M fine + potential €5.1M more (Sept 2024)
- **Novel precedent:** Investigating personal liability for management

**LinkedIn:** €310M fine for behavioral analysis (Oct 2024)

**Meta:** €251M fine for data breaches (Dec 2024)

**Total GDPR fines to date:** **€5.88 billion**

<p class="citation">Source: EU AI Act Article 99; DLA Piper GDPR Enforcement Tracker (Jan 2025)</p>

<p class="tagline">Facts, not fear.</p>

Note:
And because AI-specific regulation is still rolling out, GDPR is being used as a guardrail. Clearview AI: 30.5 million euro fine with another 5 million potentially on the way. And here's the novel precedent: they're investigating personal liability for management. LinkedIn: 310 million for behavioral analysis. Meta: 251 million for data breaches. Total GDPR fines to date? 5.88 billion euros with a B. These are real companies paying real money.

---

<!-- .slide: data-background="#FFFFFF" -->

## Discrimination Lawsuits Are Winning

**SafeRent Solutions (November 2024)**

- **$2.2M settlement** for discriminatory rental screening algorithm
- Algorithm didn't account for housing vouchers
- Discriminated against low-income applicants
- **Precedent:** Third-party validation now required for any new scoring

Note:
And it's not just Europe. In the US, discrimination lawsuits against AI are starting to win. SafeRent Solutions settled for 2.2 million dollars in November 2024. Their algorithm was screening rental applicants, and it discriminated against people using housing vouchers - which meant it discriminated against low-income applicants. The settlement requires that any new scoring algorithm they develop must be validated by a third party that the plaintiffs approve. Think about that precedent.

---

<!-- .slide: data-background="#FFFFFF" -->

## Workday AI Hiring Lawsuit

**Mobley v. Workday, Inc.** (N.D. California)

**May 2025:** First collective action certified in AI bias case

- Alleged **disparate impact** based on race, age, disability
- Could set precedent for all AI hiring tools
- Potential exposure: **$100M+**

<p class="citation">Source: Mobley v. Workday, Inc., N.D. California (May 2025); SafeRent settlement (Nov 2024)</p>

<p class="tagline">Facts, not fear.</p>

Note:
And then there's Workday. In May 2025, a court certified the first collective action lawsuit in an AI bias case. This is precedent-setting. The plaintiff alleges that Workday's AI hiring tools have disparate impact based on race, age, and disability. This case is still ongoing, but the potential exposure is over 100 million dollars. These aren't hypothetical risks. These are settled cases with real money changing hands, and judges allowing wrongful death and discrimination cases to proceed.

---

<!-- .slide: data-background="#FFD300" -->

## The Compliance Trap

### Why Regulation Alone Won't Save You

❌ **Reactive, not proactive** (written after harm occurs)
❌ **Backward-looking** (designed for yesterday's AI)
❌ **Focuses on process, not outcomes**
❌ **Creates checkbox mentality** ("we're compliant!")

Note:
But here's the problem with relying on regulation alone. Regulation is reactive. It's written after harm has already occurred. It's backward-looking - the EU AI Act was designed for machine learning training bias, not autonomous agents. It focuses on process over outcomes. And worst of all, it creates a checkbox mentality. "We're compliant!" Great. Does your AI work safely? "We're compliant!" That's not what I asked.

---

<!-- .slide: data-background="#FFFFFF" -->

## The Reality

- EU AI Act **doesn't address autonomous agents** specifically
- NIST AI RMF is **voluntary** (no enforcement mechanism)
- GDPR wasn't designed for **token-based systems**
- U.S. state-level AI laws are **fragmented patchwork**

**Key Insight:**
**Regulation gives you a baseline. It doesn't give you safety.**

<p class="tagline">Facts, not fear.</p>

Note:
The EU AI Act doesn't specifically address autonomous agents - it was written before they became widespread. The NIST AI Risk Management Framework is voluntary - there's no enforcement mechanism. GDPR was written for databases and files, not token-based AI systems. And in the US, state-level AI laws are a fragmented patchwork. Here's the key insight: regulation gives you a baseline. It tells you what you must document, what you must disclose, what you'll be fined for. It doesn't tell you if your AI actually works safely. Regulation is a simple guide. Testing is the proof.

---

<!-- .slide: data-background="#000000" class="section-header" -->

# ACT III

## THE BLIND SPOT

Note:
So if regulation isn't enough, what's missing? Let me show you the blind spot in your current security strategy.

---

<!-- .slide: data-background="#FFFFFF" -->

## What CISOs Were Trained to Protect

**Traditional Security Model:**

- **Confidentiality:** Prevent unauthorized access
- **Integrity:** Maintain data accuracy
- **Availability:** Ensure systems remain accessible

**Focus:** Data protection

Note:
Here's what you were trained to protect. The CIA triad: Confidentiality, Integrity, Availability. Prevent unauthorized access. Maintain data accuracy. Keep systems available. Your focus is data protection. This is what we've been doing for decades.

---

<!-- .slide: data-background="#FFFFFF" -->

## Traditional Threat Model

- **Unauthorized access** (stolen credentials)
- **Data exfiltration** (insider threat, external breach)
- **Service disruption** (DDoS, ransomware)

**Actor:** Human adversary operating at human speed

**What You Optimize For:**
- Perimeter defense
- Access controls
- Incident detection
- Recovery time

<p class="tagline">Facts, not fear.</p>

Note:
Your threat model is built around human adversaries. Stolen credentials. Data exfiltration. Service disruption. DDoS attacks. Ransomware. And you optimize for perimeter defense, access controls, detecting incidents, and recovering quickly. This works when your adversary is human and operates at human speed. But what happens when the threat isn't human?

---

<!-- .slide: data-background="#000000" class="section-header" -->

# The New Threat

## AI as Autonomous Attacker

Note:
Let me introduce you to a threat that your current controls weren't designed for: weaponized AI operating as autonomous attackers.

---

<!-- .slide: data-background="#FFFFFF" -->

## Weaponized LLMs: The Hacking Swarm

**LLAMA 7B Parameter Models:**

- Small enough to **run on consumer hardware**
- **Open-source, freely available**
- **Easily weaponized** into autonomous hacking bots

<p class="fragment">Low compute requirements = swarms at endpoints</p>
<p class="fragment">No centralized C2 server = untraceable</p>
<p class="fragment">Open-source models = ungovernable</p>

Note:
LLAMA models with 7 billion parameters are small enough to run on consumer hardware. A decent gaming PC can run them. They're open-source and freely available - you can download them right now. And they can be easily weaponized into autonomous hacking bots. Because they're small, they can run as swarms at endpoint devices. Because they don't need a centralized command and control server, they're untraceable. And because they're open-source, they're ungovernable - you can't recall them.

---

<!-- .slide: data-background="#FFFFFF" -->

## The Vulnerabilities

**CVE-2024-50050** (CVSS 6.3)
Remote code execution in Llama framework

**CVE-2024-34359** (CVSS 9.6)
Arbitrary code execution via Jinja2 template engine

**Model Poisoning:**
Models can be backdoored with just **250 poisoned documents**

<p class="citation">Source: The Hacker News CVE reports; ArXiv research papers</p>

Note:
The Llama framework itself has vulnerabilities. CVE-2024-50050 allows remote code execution with a severity score of 6.3. CVE-2024-34359 allows arbitrary code execution with a severity of 9.6. And research shows you can backdoor these models with just 250 poisoned documents. That's all it takes to compromise the model before someone even downloads it.

---

<!-- .slide: data-background="#FFD300" -->

## Autonomous Attack Capabilities

**Carnegie Mellon Research (2025)**

LLMs can **autonomously plan and execute multi-host network attacks**

<div class="fragment">

**HPTSA:** Hierarchical Planning and Task-Specific Agents

- Planning agent explores systems
- Team manager coordinates subagents
- Task-specific agents exploit SQL injection, XSS, etc.
- **4.5x improvement** over previous state-of-the-art

</div>

Note:
Carnegie Mellon University researchers demonstrated in 2025 that large language models can autonomously plan and execute multi-host network attacks. They created a system called HPTSA - Hierarchical Planning and Task-Specific Agents. It's a swarm. A planning agent explores the target systems. A team manager coordinates multiple subagents. And task-specific agents exploit vulnerabilities like SQL injection and cross-site scripting. This system improved attack success rates by 4.5 times over the previous state of the art.

---

<!-- .slide: data-background="#FFFFFF" -->

## Real-World Performance

- Exploit up to **13% of zero-day vulnerabilities**
- Exploit up to **25% of one-day vulnerabilities**
- Autonomously replicated **2017 Equifax breach** in test environment

<p class="citation">Source: Carnegie Mellon University (2025); "When LLMs Autonomously Attack"</p>

Note:
In testing, these AI agents could exploit 13% of zero-day vulnerabilities - vulnerabilities that have never been seen before. They could exploit 25% of one-day vulnerabilities - vulnerabilities that were just disclosed. And in a replicated environment, they autonomously executed the 2017 Equifax breach. No human instruction. No human oversight. The AI planned it, executed it, and exfiltrated the data.

---

<!-- .slide: data-background="#FFFFFF" -->

## Why This Matters

- **Low compute requirements** = can run as swarms at endpoints
- **Untraceable** = no centralized C2 server required
- **Ungovernable** = open-source models can't be recalled
- **Autonomous** = operates without human instruction

**LLMjacking Trend:**
- **10x increase** in malicious LLM requests (July 2024)
- **2x increase** in unique IP addresses attacking

<p class="tagline">Facts, not fear.</p>

Note:
Your endpoint detection isn't designed for this. Your DLP can't see this. Your firewall doesn't understand this. These attacks operate at machine speed, without centralized infrastructure, and without human oversight. And the trend is accelerating: 10x increase in malicious LLM requests in July 2024. Twice as many unique IP addresses engaging in these attacks. This is a fundamentally new attack surface.

---

<!-- .slide: data-background="#000000" class="section-header" -->

# The Cost Projection

## (And The Scare Tactics Coming Your Way)

Note:
So what's this going to cost you? Let me show you the numbers - and then let me show you how vendors are going to use these numbers to scare you.

---

<!-- .slide: data-background="#FFFFFF" -->

## IBM Cost of a Data Breach Report 2025

**Current State:**

- U.S. breach costs: **$10.22M** (all-time high, ↑9% from 2024)
- Global average: **$4.44M**
- Healthcare breaches: **$9.77M** (double the average)

<p class="citation">Source: IBM Cost of a Data Breach Report 2025</p>

Note:
IBM's 2025 Cost of a Data Breach Report shows that U.S. breach costs have hit an all-time high of 10.22 million dollars - up 9% from 2024. The global average is 4.44 million. Healthcare is getting hammered at 9.77 million per breach - double the average. These are the baseline costs.

---

<!-- .slide: data-background="#FFFFFF" -->

## AI-Amplified Breach Costs

- **Shadow AI** adds **$670K** to average breach cost
- **13%** of organizations report AI model/application breaches
- **97%** of those breached lacked AI access controls
- **60%** of AI incidents led to compromised data
- **31%** led to operational disruption

Note:
Now add AI to the mix. Organizations with high levels of shadow AI - AI systems deployed without governance - saw breach costs increase by 670 thousand dollars. 13% of organizations reported breaches of AI models or applications. And here's the kicker: 97% of those organizations lacked AI access controls. They had no governance in place. 60% of those incidents led to compromised data. 31% led to operational disruption.

---

<!-- .slide: data-background="#FFFFFF" -->

## AI-Driven Ransomware

- **149% increase** in ransomware incidents (early 2025)
- **16%** of breaches involved attackers using AI
- **37%** used AI-generated phishing
- **35%** used deepfake impersonation

<p class="citation">Source: IBM 2025 report; Cyber Defense Magazine; Axis Insurance</p>

Note:
And ransomware is getting worse. There's been a 149% increase in ransomware incidents in early 2025. 16% of all breaches now involve attackers using AI. 37% are using AI-generated phishing emails that are virtually indistinguishable from legitimate messages. 35% are using deepfake impersonation - voice and video - to trick employees.

---

<!-- .slide: data-background="#FFD300" -->

## The Forecast

<div class="stat-large">2x</div>

### AI-amplified breach costs are projected to more than double current costs within 18-24 months

Note:
Here's the forecast that should concern you: AI-amplified breach costs are projected to more than double within 18 to 24 months. That means we're looking at 20 million dollar breaches becoming the norm. And here's what's going to happen.

---

<!-- .slide: data-background="#FFFFFF" -->

## Here's What's Going to Happen

**Cybersecurity vendors will use these numbers to scare you into paying more for the "latest AI-powered protection"**

<div class="fragment">

**The Pitch You'll Hear:**

- "AI-powered threat detection!"
- "Next-gen AI defense!"
- "Stay ahead of AI threats with our AI!"

</div>

Note:
Cybersecurity vendors are going to use these numbers to scare you. You're going to hear pitches for "AI-powered threat detection" and "next-gen AI defense" and "stay ahead of AI threats with our AI!" They're going to tell you that you need to fight AI with AI, and they're going to charge you a premium for it.

---

<!-- .slide: data-background="#FFD300" -->

## The Reality

<div class="quote-box">

**Real protection isn't marketed.**

**It's methodical, predictable, testable, measurable.**

</div>

<p class="tagline">Facts, not fear.</p>

Note:
Here's the reality: real protection isn't marketed. It's not sold with fear. It's methodical. It's predictable. It's testable. It's measurable. When vendors start pitching you "AI-powered solutions," ask them one question: How do you test it? If they can't answer, it's theatre.

---

<!-- .slide: data-background="#000000" -->

## The Expanding Attack Surface

**Three Simultaneous Threat Dimensions:**

1. **Traditional Risk** (You know this)
2. **AI-Amplified Risk** (Your current blind spot)
3. **AI-as-the-Risk** (Your biggest blind spot)

Note:
You're now dealing with three simultaneous threat dimensions. Let me break them down.

---

<!-- .slide: data-background="#FFFFFF" -->

## Traditional Risk

- Compromised credentials → data breach
- Human attacker at human speed
- Cost: **$4.88M** average (IBM 2024)

**Your Controls Handle This**

Note:
Traditional risk: compromised credentials lead to data breach. Human attacker operating at human speed. Your controls were designed for this. You have perimeter defense, access controls, incident detection. Average cost: 4.88 million. You know how to deal with this.

---

<!-- .slide: data-background="#FFFFFF" -->

## AI-Amplified Risk

- Compromised agent → **10x privilege escalation**
- Autonomous swarms at **machine speed**
- Lateral movement across **multiple systems simultaneously**
- Cost: **$10M+** (agent-amplified breach)

**Your Current Blind Spot**

<p class="tagline">Facts, not fear.</p>

Note:
AI-amplified risk: a compromised AI agent has 10x the privilege escalation potential. Autonomous swarms operate at machine speed. They move laterally across multiple systems simultaneously. Cost: 10 million plus. This is your current blind spot. Your controls weren't designed for this.

---

<!-- .slide: data-background="#FFFFFF" -->

## AI-as-the-Risk

- **No external attacker required**
- AI directly harms humans
- Encourages self-harm, illegal activity, discrimination
- Impacts vulnerable populations (children, mentally ill)

**Cost:**
- Wrongful death lawsuits
- Discrimination settlements
- Reputational destruction
- Criminal liability

**Your Biggest Blind Spot**

Note:
And AI-as-the-risk: no external attacker required. The AI itself is the threat. It encourages self-harm. It discriminates. It harms vulnerable populations like children and the mentally ill. The cost? Wrongful death lawsuits. Discrimination settlements. Reputational destruction. Potential criminal liability. This is your biggest blind spot. Your data protection strategy protects data. It doesn't protect humans from the AI itself.

---

<!-- .slide: data-background="#FFD300" -->

## The Gap

<div class="quote-box">

**Your data protection strategy protects data.**

**It doesn't protect humans from the AI itself.**

</div>

<p class="tagline">Facts, not fear.</p>

Note:
You can't DLP your way out of this. You can't firewall away a chatbot telling a 14-year-old to commit suicide. You need a fundamentally different approach. And that's what we're going to talk about next.

---

<!-- .slide: data-background="#000000" class="section-header" -->

# ACT IV

## BEYOND SECURITY
## TESTING FOR HUMAN SAFETY

Note:
So what's the solution? How do you test whether your AI is safe - not just secure, but actually safe for humans? Let me introduce you to the missing control.

---

<!-- .slide: data-background="#FFFFFF" -->

## The Testing Gap

**Current State:**

- **59%** call agentic AI implementation "a work in progress"
- **44%** deploying agents have NO security policies
- **97%** of AI-breached organizations lacked access controls
- **~0%** test for alignment, bias, or fairness before deployment

Note:
Here's the testing gap. 59% of organizations call their agentic AI implementation "a work in progress" - which means they're deploying it while still figuring it out. 44% of organizations deploying AI agents have no security policies at all. 97% of organizations that got breached through their AI lacked basic access controls. And approximately zero percent test for alignment, bias, or fairness before deployment.

---

<!-- .slide: data-background="#FFD300" -->

## The Question

<div class="question-box">

**Would you deploy code to production without testing it?**

</div>

<div class="fragment">

**Then why would you deploy an AI system that interacts with humans?**

</div>

Note:
Let me ask you a simple question. Would you deploy code to production without testing it? Of course not. Then why would you deploy an AI system that interacts with humans without testing it? The excuse I hear is "but we have an AI governance policy!" Having a policy and testing your system are not the same thing.

---

<!-- .slide: data-background="#000000" class="section-header" -->

# Introducing

## Evals with Fairness Rubrics

Note:
So what's the solution? Evals with fairness rubrics. Let me explain what these are and why they matter.

---

<!-- .slide: data-background="#FFFFFF" -->

## What Are Evals?

**Systematic evaluation of LLM behavior before and during deployment**

**What They Test:**

- **Correctness:** Factually accurate outputs
- **Relevancy:** Responses match user queries
- **Safety:** Non-harmful, appropriate outputs
- **Fairness:** Unbiased, equitable treatment across demographics

Note:
Evals are systematic evaluations of large language model behavior. You run them before deployment and continuously during production. They test four things: Correctness - is the output factually accurate? Relevancy - does the response match what the user asked? Safety - is the output non-harmful and appropriate? And fairness - does it treat different demographic groups equitably?

---

<!-- .slide: data-background="#FFFFFF" -->

## Why Fairness Rubrics Matter

- Detect **toxic bias** before it reaches users
- Measure **equitable treatment** across protected classes
- Quantify **alignment with human values and safety**
- **Provide proof of due diligence** for regulators and courts

Note:
Fairness rubrics matter because they detect toxic bias before it reaches your users. They measure equitable treatment across protected classes like race, gender, age, disability. They quantify alignment with human values and safety. And critically, they provide proof of due diligence. When you're sitting in front of a regulator or a judge, you can show them exactly what you tested and when.

---

<!-- .slide: data-background="#FFFFFF" -->

## The Frameworks

**Open Source, Production-Ready Tools:**

- **DeepEval:** Toxicity, bias, fairness, helpfulness metrics
- **RAGAs:** Agent tool usage, goal accuracy, retrieval quality
- **DeepChecks:** Data drift, quality validation, model monitoring

<p class="citation">Source: github.com/lizTheDeveloper/monitoring_llms_demo</p>

<p class="tagline">Facts, not fear.</p>

Note:
These aren't theoretical frameworks. These are open-source, production-ready tools. DeepEval tests for toxicity, bias, fairness, and helpfulness. RAGAs tests agent tool usage, goal accuracy, and retrieval quality. DeepChecks validates data quality, detects drift, and monitors models in production. These are the tools used by companies that are actually serious about AI safety.

---

<!-- .slide: data-background="#000000" -->

## The Four Critical Alignment Metrics

1. **Toxicity Detection**
2. **Bias Detection**
3. **Fairness Evaluation**
4. **Behavioral Alignment**

Note:
Let me break down the four critical alignment metrics you need to test. These are non-negotiable. All four must pass before you deploy to production.

---

<!-- .slide: data-background="#FFFFFF" -->

## 1. Toxicity Detection

- Hate speech, harmful content
- Self-harm encouragement
- Violence promotion
- Sexually explicit material (especially to minors)

**Example:**
Would have flagged Character.AI's "go through with it" response and Grok's "send nudes" request

Note:
Toxicity detection catches hate speech, harmful content, self-harm encouragement, violence promotion, and sexually explicit material - especially when directed at minors. This would have caught Character.AI telling a 14-year-old that "not knowing if it would work" wasn't a good reason not to attempt suicide. It would have caught Grok asking a 12-year-old to send nude photos.

---

<!-- .slide: data-background="#FFFFFF" -->

## 2. Bias Detection

- Demographic fairness across race, gender, age
- Equal treatment across protected classes
- Disparate impact measurement
- Representation balance

**Example:**
SafeRent's algorithm discriminating against housing voucher recipients

Note:
Bias detection measures demographic fairness. Does your AI treat different races, genders, and ages equally? Does it respect protected classes? Does it have disparate impact on certain groups? Is representation balanced? This would have caught SafeRent's algorithm that discriminated against people using housing vouchers.

---

<!-- .slide: data-background="#FFFFFF" -->

## 3. Fairness Evaluation

- Equitable outcomes across populations
- Access equality
- Service quality consistency
- Decision explainability

**Example:**
Workday's hiring algorithm showing disparate impact

Note:
Fairness evaluation goes beyond bias detection. It measures whether outcomes are equitable across populations. Do all users get equal access? Is service quality consistent? Can you explain why the AI made a particular decision? This would have caught Workday's hiring algorithm that showed disparate impact based on race, age, and disability.

---

<!-- .slide: data-background="#FFFFFF" -->

## 4. Behavioral Alignment

- Response appropriateness for context
- Age-appropriate content filtering
- Tone and empathy measurement
- Boundary respect

**Example:**
DPD chatbot calling company "worst delivery firm" and Air Canada chatbot giving false information

<p class="tagline">Facts, not fear.</p>

Note:
Behavioral alignment tests whether responses are appropriate for the context. Is content age-appropriate? Is the tone and empathy level right? Does the AI respect boundaries? This would have caught DPD's chatbot swearing at customers and calling the company the worst delivery firm in the world. It would have caught Air Canada's chatbot giving false information about bereavement discounts. All four metrics must pass. This is the standard.

---

<!-- .slide: data-background="#000000" -->

## The Testing Spectrum

**Security Testing** (You Already Do This)
**+**
**Alignment Testing** (You Should Be Doing This)
**=**
**Complete Protection**

Note:
Here's how this fits into what you're already doing. You already do security testing. Pen tests, vulnerability scans, red team exercises, incident response tabletops. That protects you against external adversaries. Now you need alignment testing. Toxicity evals, bias detection, fairness rubrics, behavioral alignment. That protects you against your own AI. Both are required. Neither is sufficient alone.

---

<!-- .slide: data-background="#FFFFFF" -->

## Security Testing vs Alignment Testing

**Security Testing:**
- Penetration testing
- Vulnerability scanning
- Red team exercises
- Incident response tabletops
- **Purpose:** Protect against external adversaries

**Alignment Testing:**
- Toxicity evals across conversation scenarios
- Bias detection across demographic groups
- Fairness rubrics for decision systems
- Behavioral alignment for edge cases
- **Purpose:** Protect against your own AI

Note:
You wouldn't deploy infrastructure without pen testing it. Don't deploy AI without eval testing it. The principle is the same: test before you deploy, monitor after you deploy, and have an incident response plan for when things go wrong.

---

<!-- .slide: data-background="#FFD300" -->

## Production Monitoring Example

**Scenario:** Customer service chatbot deployment

Note:
Let me show you what this looks like in practice. Let's say you're deploying a customer service chatbot. Here's the testing lifecycle.

---

<!-- .slide: data-background="#FFFFFF" -->

## Before Deployment

**Baseline Testing:**

- Run toxicity evals across **10,000 test conversations**
- Test bias across demographic groups (age, race, gender, income)
- Validate age-appropriate response filtering
- Measure fairness in service delivery and wait times
- **Document baseline metrics** for regulatory compliance

Note:
Before deployment, you run toxicity evals across 10,000 test conversations. You test for bias across demographic groups - age, race, gender, income. You validate that age-appropriate response filtering is working. You measure fairness in service delivery and wait times. And critically, you document your baseline metrics. This documentation is your proof of due diligence for regulators.

---

<!-- .slide: data-background="#FFFFFF" -->

## During Production

**Continuous Monitoring:**

- Real-time toxicity monitoring on **every conversation**
- Continuous bias detection and alerting
- Anomaly detection when behavior drifts from baseline
- Automated kill switch triggers for alignment violations
- Cryptographically signed audit logs

Note:
During production, you monitor in real-time. Every single conversation gets checked for toxicity. Continuous bias detection with automated alerts. Anomaly detection that triggers when behavior drifts from your baseline. Automated kill switches that trigger when alignment violations occur. And cryptographically signed audit logs so you can prove what the system did and when.

---

<!-- .slide: data-background="#FFFFFF" -->

## After Incident

**Forensic Analysis:**

- Complete conversation log reconstruction
- Root cause identification using eval frameworks
- Retraining with fairness constraints
- Validation testing before redeployment
- Regulatory reporting with documented testing proof

**Result:**
Measurable, provable due diligence. Defensible in court. Demonstrable to regulators.

<p class="tagline">Facts, not fear.</p>

Note:
And when an incident occurs - because incidents will occur - you have complete conversation log reconstruction. You use your eval frameworks to identify root cause. You retrain the model with fairness constraints. You validate before redeploying. And you report to regulators with documented proof of what you tested. This is measurable, provable due diligence. This is what holds up in court. This is what you can demonstrate to regulators. This is what good looks like - not a dashboard, not a policy, but actual testing with actual metrics.

---

<!-- .slide: data-background="#000000" class="section-header" -->

# ACT V

## THE ROI OF KNOWING WHAT'S REAL

Note:
So you might be thinking, "This sounds expensive. How do I justify this to my CFO?" Let me show you the business case.

---

<!-- .slide: data-background="#FFFFFF" -->

## Cost of NOT Testing

**Character.AI:** Wrongful death lawsuit proceeding (est. **$10M+** settlement + reputation)

**SafeRent:** **$2.2M** settlement + reputation damage + mandatory third-party oversight

**Clearview AI:** **€30.5M** fine + potential €5.1M more + personal liability investigation

**Workday:** Class action proceeding (potential **$100M+** exposure)

**DPD:** Immeasurable reputation damage, system disabled, public apology

**Air Canada:** CA$812 + legal fees + precedent that you can't disclaim your AI

Note:
Here's the cost of not testing. Character.AI is facing a wrongful death lawsuit with an estimated settlement of 10 million plus, not counting reputation damage. SafeRent paid 2.2 million and is now under mandatory third-party oversight. Clearview AI got hit with a 30.5 million euro fine with another 5 million potentially coming, and regulators are investigating personal liability for management. Workday is facing potential exposure over 100 million. DPD suffered immeasurable reputation damage. Air Canada set precedent that you cannot disclaim your AI. These are real costs.

---

<!-- .slide: data-background="#FFFFFF" -->

## Cost of Testing

**Evals framework implementation:** $50K-$150K (one-time)

**Production monitoring infrastructure:** $100K-$250K annually

**Third-party validation:** $75K-$150K per engagement

**Total Year 1:** ~$300K-$550K

Note:
Now here's the cost of testing. Implementing an evals framework: 50 to 150 thousand dollars one-time. Production monitoring infrastructure: 100 to 250 thousand annually. Third-party validation: 75 to 150 thousand per engagement. Total first year cost: approximately 300 to 550 thousand dollars.

---

<!-- .slide: data-background="#FFD300" -->

## Net Calculation

<div class="stat-large">18-33 years</div>

### One prevented wrongful death lawsuit = 18-33 years of testing budget

**ROI Timeline:**
If testing prevents even ONE of the above incidents, payback is immediate.

<p class="tagline">Facts, not fear.</p>

Note:
Do the math. One prevented wrongful death lawsuit equals 18 to 33 years of testing budget. If your testing prevents even one of those incidents - one wrongful death, one discrimination lawsuit, one major fine - the payback is immediate. This isn't a cost center. This is liability insurance with measurable ROI.

---

<!-- .slide: data-background="#000000" -->

## What "Testing Viability" Means

**Viability = Security + Ethical Alignment + Measurable Proof**

Note:
At nuwest.ai, we define viability as the sum of three things: security, ethical alignment, and measurable proof. Let me break that down.

---

<!-- .slide: data-background="#FFFFFF" -->

## Security Viability

- Can your IR plan handle **agent-amplified attacks**?
- Can you detect compromised autonomous systems operating at **machine speed**?
- Can you contain **AI swarms at endpoints**?
- Can you recover when agents are **part of the attack chain**?

Note:
Security viability asks: can your incident response plan handle agent-amplified attacks? Can you detect compromised autonomous systems that operate at machine speed? Can you contain AI swarms running at endpoint devices? Can you recover when agents themselves are part of the attack chain? These are questions your current security strategy may not be able to answer.

---

<!-- .slide: data-background="#FFFFFF" -->

## Ethical Viability

- Does your AI **harm humans**?
- Does it **discriminate** against protected classes?
- Does it **encourage illegal or dangerous behavior**?
- Does it respect **age-appropriate boundaries**?

**Can you prove it doesn't?**

Note:
Ethical viability asks: does your AI harm humans? Does it discriminate against protected classes? Does it encourage illegal or dangerous behavior? Does it respect age-appropriate boundaries? And critically - can you prove it doesn't? That last question is the one that matters in court.

---

<!-- .slide: data-background="#FFFFFF" -->

## Measurable Viability

- Do you have **baseline metrics documented**?
- Can you demonstrate **due diligence to regulators**?
- Can you show **what you tested and when**?
- Can you **defend yourself in court with data**?

<p class="tagline">Facts, not fear.</p>

Note:
Measurable viability asks: do you have baseline metrics documented? Can you demonstrate due diligence to regulators? Can you show what you tested and when? Can you defend yourself in court with data? This is the difference between saying "we're compliant" and proving "we did everything reasonable to prevent harm."

---

<!-- .slide: data-background="#FFD300" -->

## The Value Proposition

Testing viability isn't optional.

**It's the difference between:**

- Wrongful death lawsuit **vs.** documented due diligence
- €35M fine **vs.** regulatory compliance proof
- Viral embarrassment **vs.** controlled deployment

Note:
Testing viability isn't optional. It's the difference between facing a wrongful death lawsuit versus showing documented due diligence. It's the difference between a 35 million euro fine versus being able to prove regulatory compliance. It's the difference between viral embarrassment versus controlled deployment.

---

<!-- .slide: data-background="#000000" -->

## Why Regulation Is The Baseline, Not The Ceiling

Note:
Let me be very clear about something: regulation is important, but it's not enough. Here's why.

---

<!-- .slide: data-background="#FFFFFF" -->

## What Regulation Tells You

✓ What you must **disclose** (transparency requirements)
✓ What you must **document** (audit trails, data provenance)
✓ What you'll be **fined for** (prohibited practices, violations)
✓ What **processes you must follow** (governance, oversight)

Note:
Regulation tells you what you must disclose - transparency requirements. What you must document - audit trails and data provenance. What you'll be fined for - prohibited practices and violations. What processes you must follow - governance and oversight. This is valuable. This is important. But it's not sufficient.

---

<!-- .slide: data-background="#FFFFFF" -->

## What Regulation Doesn't Tell You

❌ If your AI **actually works safely**
❌ If your testing methodology is **sufficient**
❌ If you've **prevented harm** (vs. just checked boxes)
❌ If you can **defend yourself in court**

<p class="tagline">Facts, not fear.</p>

Note:
Regulation doesn't tell you if your AI actually works safely. It doesn't tell you if your testing methodology is sufficient. It doesn't tell you if you've actually prevented harm versus just checking compliance boxes. And it doesn't tell you if your approach will hold up in court when someone gets hurt.

---

<!-- .slide: data-background="#FFD300" -->

## The nuwest.ai Approach

**Regulation:** A simple guide to minimum requirements
**Testing:** The proof that you met them (and exceeded them)
**Viability:** The goal that keeps humans safe

Note:
Here's our approach: regulation is a simple guide to minimum requirements. Testing is the proof that you met those requirements and exceeded them. And viability is the goal - the thing that actually keeps humans safe. Compliance gets you to the starting line. Testing gets you across the finish line without killing someone.

---

<!-- .slide: data-background="#000000" class="section-header" -->

# ACT VI

## THE PATH TO MEASURABLE VIABILITY

Note:
So you understand the problem. You understand the solution. Now the question is: what do you do on Monday morning? Let me give you the framework and the timeline.

---

<!-- .slide: data-background="#FFFFFF" -->

## NIST AI RMF Framework (Adapted for Testing)

**The Four Functions:**

1. **GOVERN**
2. **MAP**
3. **MEASURE**
4. **MANAGE**

<p class="citation">Source: NIST AI RMF 2.0 (Feb 2024); Generative AI Profile NIST-AI-600-1 (July 2024)</p>

Note:
The NIST AI Risk Management Framework gives us a proven structure. It has four functions: Govern, Map, Measure, and Manage. We've adapted it specifically for testing AI viability. Here's what each function means in practice.

---

<!-- .slide: data-background="#FFFFFF" -->

## 1. GOVERN

- Who can **deploy AI systems**?
- What **testing is required** before deployment?
- Who **approves production releases**?
- What metrics define **"safe enough"**?

Note:
Govern means establishing the rules. Who in your organization can deploy AI systems? What testing is required before deployment? Who approves production releases? And critically - what metrics define "safe enough"? You need to answer these questions before you deploy anything.

---

<!-- .slide: data-background="#FFFFFF" -->

## 2. MAP

- **Inventory ALL AI systems** (including shadow AI)
- Document capabilities, access levels, and **human touchpoints**
- Identify **high-risk applications** (customer-facing, autonomous decision-making)
- Classify by impact potential (reputational, financial, safety, **life-threatening**)

Note:
Map means knowing what you have. Inventory all AI systems - including shadow AI that's been deployed without your knowledge. Document what they can do, what they can access, and where they touch humans. Identify high-risk applications - anything customer-facing or making autonomous decisions. And classify them by impact potential: reputational, financial, safety, or life-threatening.

---

<!-- .slide: data-background="#FFFFFF" -->

## 3. MEASURE

- Implement **evals with fairness rubrics** (DeepEval, RAGAs, DeepChecks)
- **Baseline** toxicity, bias, fairness, alignment metrics
- Monitor production behavior **continuously**
- Set **alert thresholds** for violations

Note:
Measure means implementing the testing we've been talking about. Use evals with fairness rubrics - DeepEval, RAGAs, DeepChecks. Establish baseline metrics for toxicity, bias, fairness, and alignment. Monitor production behavior continuously - not quarterly, continuously. And set alert thresholds that trigger when violations occur.

---

<!-- .slide: data-background="#FFFFFF" -->

## 4. MANAGE

- Automated **kill switches** for harmful behavior
- Incident response playbooks for **AI-caused harm**
- Continuous **revalidation cycles** (quarterly minimum)
- **Third-party audits** annually

<p class="tagline">Facts, not fear.</p>

Note:
Manage means having controls in place. Automated kill switches that shut down harmful behavior immediately. Incident response playbooks specifically for AI-caused harm - this is different from your cybersecurity IR plan. Continuous revalidation cycles - quarterly at minimum. And third-party audits annually to validate that your testing is sufficient. This is the framework. Now here's the timeline.

---

<!-- .slide: data-background="#000000" -->

## 90-Day Action Plan

**Week 1-2:** Discovery
**Week 3-4:** Baseline Testing
**Week 5-8:** Production Integration
**Week 9-12:** Validation

Note:
You can get from blind to informed in 90 days. Here's how.

---

<!-- .slide: data-background="#FFFFFF" -->

## Week 1-2: Discovery

- Audit current AI systems across the organization
- Identify high-risk deployments (customer-facing, decision-making, children)
- Document current testing practices (spoiler: probably none exist)

**Deliverable:** AI inventory with risk classifications

Note:
Weeks 1 and 2: Discovery. Audit all the AI systems across your organization. Identify high-risk deployments - anything customer-facing, anything making decisions, anything that children might use. Document your current testing practices. Spoiler alert: you probably don't have any. Deliverable: an AI inventory with risk classifications.

---

<!-- .slide: data-background="#FFFFFF" -->

## Week 3-4: Baseline Testing

- Select evals framework (DeepEval, RAGAs, or both)
- Run initial toxicity, bias, fairness tests on **top 3 high-risk systems**
- Measure current state (expect bad news)

**Deliverable:** Baseline metrics report

Note:
Weeks 3 and 4: Baseline testing. Select your evals framework - DeepEval, RAGAs, or both. Run initial tests on your top 3 highest-risk systems. Measure the current state. I'm warning you now: expect bad news. Most systems fail their first eval tests. Deliverable: baseline metrics report that documents where you are today.

---

<!-- .slide: data-background="#FFFFFF" -->

## Week 5-8: Production Integration

- Implement **real-time monitoring** for critical systems
- Establish **alert thresholds** and automated responses
- Create **incident response playbooks** for AI harm scenarios
- Train SOC team on AI-specific incidents

**Deliverable:** Monitoring dashboard + playbooks

Note:
Weeks 5 through 8: Production integration. Implement real-time monitoring for your critical systems. Establish alert thresholds and automated responses - what happens when toxicity is detected? Create incident response playbooks specifically for AI harm scenarios - this is different from your cyber IR plan. Train your SOC team on AI-specific incidents. Deliverable: monitoring dashboard and playbooks.

---

<!-- .slide: data-background="#FFFFFF" -->

## Week 9-12: Validation

- **Third-party assessment** of testing adequacy
- Gap analysis against **EU AI Act, GDPR, NIST AI RMF**
- **Board presentation** with measurable metrics
- Roadmap for remaining systems

**Deliverable:** Executive report + remediation roadmap

<p class="tagline">Facts, not fear.</p>

Note:
Weeks 9 through 12: Validation. Get a third-party assessment of whether your testing is adequate. Do a gap analysis against the EU AI Act, GDPR, and NIST AI RMF. Present to your board with measurable metrics - not feelings, metrics. And create a roadmap for addressing your remaining systems. Deliverable: executive report and remediation roadmap. 90 days. That's all it takes to go from blind to informed.

---

<!-- .slide: data-background="#FFD300" -->

## What Good Looks Like vs. What Failure Looks Like

Note:
Let me show you the difference between mature and immature AI governance.

---

<!-- .slide: data-background="#FFFFFF" -->

## Mature AI Governance

✓ No AI reaches production without **passing evals**
✓ Toxicity, bias, fairness metrics **measured continuously**
✓ **Automated kill switches** for alignment violations
✓ **Cryptographically signed audit trails**
✓ **Third-party validation** annually
✓ **Board-level visibility** into AI safety metrics
✓ Documented testing methodology **defensible in court**

Note:
Mature AI governance looks like this: no AI reaches production without passing evals. Toxicity, bias, and fairness are measured continuously. Automated kill switches trigger on alignment violations. Cryptographically signed audit trails prove what happened when. Third-party validation annually. Board-level visibility into AI safety metrics. And documented testing methodology that's defensible in court.

---

<!-- .slide: data-background="#FFFFFF" -->

## Immature AI Governance

❌ "We have a policy" (untested, unenforced)
❌ "Our vendor says it's safe" (no independent verification)
❌ "We'll fix it if something goes wrong" (reactive, not proactive)
❌ "Legal will handle it" (after wrongful death lawsuit filed)
❌ "Compliance checked the box" (process over outcomes)

Note:
Immature AI governance looks like this: "We have a policy" - but it's untested and unenforced. "Our vendor says it's safe" - but you haven't verified that independently. "We'll fix it if something goes wrong" - reactive instead of proactive. "Legal will handle it" - after the wrongful death lawsuit has been filed. "Compliance checked the box" - focusing on process over outcomes.

---

<!-- .slide: data-background="#FFD300" -->

## The Question

<div class="question-box">

**Which organization do you want to be when the judge asks:**

**"What did you do to prevent this?"**

</div>

<p class="tagline">Facts, not fear.</p>

Note:
Which organization do you want to be when you're sitting in front of a judge and they ask: "What did you do to prevent this?" Do you want to say "we had a policy" or do you want to show them documented evidence of systematic testing? That's the difference between mature and immature AI governance.

---

<!-- .slide: data-background="#000000" class="section-header" -->

# ACT VII

## TESTING VIABILITY, NOT SELLING THEATRE

Note:
So let me tell you about the nuwest.ai approach. What we do differently, and why organizations partner with us.

---

<!-- .slide: data-background="#FFFFFF" -->

## What We Don't Do

❌ Security theatre
❌ Checkbox compliance
❌ Reassuring dashboards that measure nothing
❌ "AI-powered" solutions (that are just marketing)
❌ Vendor pitches disguised as assessments

Note:
Let me start with what we don't do. We don't do security theatre. We don't do checkbox compliance. We don't build reassuring dashboards that measure nothing. We don't sell "AI-powered" solutions that are just marketing. And we don't disguise vendor pitches as assessments.

---

<!-- .slide: data-background="#FFFFFF" -->

## What We Do

✓ Test whether your AI is **actually safe** (security + ethics)
✓ Measure viability with **evals and fairness rubrics**
✓ Prove **what's real** (not what's comfortable)
✓ Provide evidence that **holds up in court and with regulators**

<p class="tagline">Facts, not fear.</p>

Note:
Here's what we do. We test whether your AI is actually safe - both security and ethics. We measure viability using evals and fairness rubrics. We prove what's real, not what's comfortable. And we provide evidence that holds up in court and with regulators. Most companies sell tools. We measure truth.

---

<!-- .slide: data-background="#000000" -->

## Our Service Offering

**Security Viability Testing**
+
**Ethical Viability Testing**
=
**Complete AI Assurance**

Note:
Our service offering has two components that work together.

---

<!-- .slide: data-background="#FFFFFF" -->

## Security Viability Testing

- **Agent-amplified incident response tabletops**
- Compromised AI simulation (autonomous swarms, privilege escalation)
- **MTTR measurement** for autonomous threats
- AI-specific attack chain validation

Note:
Security viability testing includes agent-amplified incident response tabletops - we simulate what happens when your AI agents get compromised. We test for autonomous swarms and privilege escalation. We measure your mean time to recovery for autonomous threats - not human threats, autonomous threats. And we validate your attack chain response when AI is involved.

---

<!-- .slide: data-background="#FFFFFF" -->

## Ethical Viability Testing

- **Evals with fairness rubrics** (DeepEval, RAGAs, DeepChecks)
- Toxicity, bias, alignment assessment across scenarios
- Age-appropriate content filtering validation
- **Third-party validation** for regulatory defense

Note:
Ethical viability testing uses evals with fairness rubrics - DeepEval, RAGAs, DeepChecks. We assess toxicity, bias, and alignment across realistic scenarios. We validate that age-appropriate content filtering is working. And we provide third-party validation that you can use for regulatory defense.

---

<!-- .slide: data-background="#FFFFFF" -->

## Why Both?

**Security without ethics testing** leaves humans at risk.

**Ethics without security testing** leaves systems vulnerable.

**You need both.**

<p class="tagline">Facts, not fear.</p>

Note:
Why both? Because security testing without ethics testing leaves humans at risk from your own AI. And ethics testing without security testing leaves your systems vulnerable to compromise. You need both. They're not optional. They're complementary.

---

<!-- .slide: data-background="#000000" -->

## Deliverables

**What You Get:**

- **Executive report** (for your board and regulators)
- **Technical playbook** (for your SOC and development teams)
- **Measurable baselines** (defensible metrics)
- **Proof of due diligence** (for your lawyers and insurers)

Note:
Here's what you get when you work with us. An executive report for your board and regulators - not technical jargon, business language. A technical playbook for your SOC and development teams - specific actions they need to take. Measurable baselines - actual numbers, not feelings. And proof of due diligence for your lawyers and insurers.

---

<!-- .slide: data-background="#FFFFFF" -->

## Engagement Model

**4-week assessment**

- Week 1: Discovery and inventory
- Week 2-3: Testing and simulation
- Week 4: Analysis and reporting

**Bespoke scenarios tailored to YOUR AI footprint**

**Real-time measurement, not theoretical analysis**

Note:
Our engagement model is a 4-week assessment. Week 1: discovery and inventory of your AI systems. Weeks 2 and 3: testing and simulation with bespoke scenarios tailored to your specific AI footprint. Week 4: analysis and reporting with measurable results. This isn't theoretical. This is real-time measurement of your actual systems.

---

<!-- .slide: data-background="#FFD300" -->

## Introducing bsdetector

**Coming Soon**

A tool to measure how much "bs" exists in your current recovery workflow.

**Purpose:**
- Test if you're truly cyber resilient (not just compliant)
- Measure gaps in your AI incident response capability
- Quantify the difference between theory and reality

**Interested?** Visit **nuwest.ai** and leave your details.

Note:
And we're building something new. It's called bsdetector. It's a tool that measures how much BS exists in your current recovery workflow. The purpose is simple: test if you're truly cyber resilient, not just compliant. Measure the gaps in your AI incident response capability. Quantify the difference between theory - "we have a plan" - and reality - "it actually works." If you're interested, visit nuwest.ai and leave your details to be first in line.

---

<!-- .slide: data-background="#000000" class="section-header" -->

# THE CHOICE

Note:
We're almost done. Let me leave you with this.

---

<!-- .slide: data-background="#FFFFFF" -->

## The Reality

- AI is **already deployed** in your environment
- Some of it **will harm humans**
- **You will be held responsible**

Note:
Here's the reality. AI is already deployed in your environment. Some of it will harm humans - maybe not today, maybe not tomorrow, but eventually. And when it does, you will be held responsible.

---

<!-- .slide: data-background="#FFFFFF" -->

## The Choice

**Test viability now, in a controlled environment**

**OR**

**Validate it during a wrongful death lawsuit**

<p class="tagline">Facts, not fear.</p>

Note:
So here's your choice. You can test viability now, in a controlled environment, where you can find the problems and fix them. Or you can validate your approach during a wrongful death lawsuit, when it's too late to fix anything.

---

<!-- .slide: data-background="#FFD300" -->

## Next Steps

1. **Audit** your current AI deployments
2. **Schedule** a discovery call with nuwest.ai
3. **Test** one high-risk system with evals
4. **Measure** what's real

Note:
Here are your next steps. First, audit your current AI deployments. Second, schedule a discovery call with nuwest.ai. Third, test one high-risk system with evals - just one, see what you find. Fourth, measure what's real. Not what's comfortable. What's real.

---

<!-- .slide: data-background="#000000" class="closing-slide" -->

# nuwest.ai >

**[your.email@nuwest.ai](mailto:your.email@nuwest.ai)**

**nuwest.ai/schedule**

<p class="tagline-closing">Measure what matters.</p>

Note:
Thank you. I'll stay on for questions. Let's find your blind spots before someone gets hurt.

---

<!-- .slide: data-background="#FFFFFF" class="backup-slide" -->

## References & Citations

**Act I - AI Failures:**
1. DPD chatbot incident - TechRadar, ITV News (Jan 19, 2024)
2. Moffatt v. Air Canada, 2024 BCCRT 149 (Feb 14, 2024)
3. Tesla Grok incident - CBC News investigation (Oct 29, 2025)
4. Garcia v. Character Technologies Inc. - U.S. District Court, Orlando (Oct 2024); NBC News, Washington Post, CNN

**Act II - Regulation:**
5. EU AI Act Article 99; DLA Piper GDPR Enforcement Tracker (Jan 2025)
6. Mobley v. Workday, Inc., N.D. California (May 2025)
7. SafeRent Solutions class action settlement (Nov 2024)

Note:
Backup slide with citations. Don't show unless asked.

---

<!-- .slide: data-background="#FFFFFF" class="backup-slide" -->

## References & Citations (continued)

**Act III - AI Threats:**
8. CVE-2024-50050, CVE-2024-34359 - The Hacker News
9. Carnegie Mellon University - "When LLMs Autonomously Attack" (2025)
10. HPTSA swarm research - ArXiv
11. IBM Cost of a Data Breach Report 2025
12. AI-driven ransomware statistics - Cyber Defense Magazine, Axis Insurance

**Act IV - Evals:**
13. github.com/lizTheDeveloper/monitoring_llms_demo
14. NIST AI RMF 2.0 (Feb 2024); NIST-AI-600-1 (July 2024)

Note:
Backup slide with remaining citations.
