<!-- .slide: data-background="#000000" class="title-slide" -->

# THE AI GOVERNANCE BLIND SPOT

## Why Your Data Protection Strategy Isn't Ready for Machine Learning

**CISO Virtual Forum**
November 12, 2025

<p class="logo-title">nuwest.ai <span class="logo-chevron">>></span></p>

**Neil Ashworth**
Founder & Chief Architect

Note:
Welcome to the CISO Virtual Forum. I'm here to talk about something most organizations don't realize they have: a blind spot in their AI governance that's creating new liability - not just for data breaches, but for human harm. By the end of this presentation, you'll understand why your current data protection strategy isn't designed for the AI systems you're already deploying, and what you need to do about it before someone gets hurt - or worse.

---

<!-- .slide: data-background="#FFFFFF" -->

## Act I: When AI Fails

**Four Examples of Escalating Harm**

- Reputational Damage (DPD)
- Financial Liability (Air Canada)
- Child Safety (Tesla Grok)
- Loss of Life (Character.AI)

Note:
Let me start with four real-world examples that show the escalating consequences when AI systems fail. Each one is worse than the last. Each was preventable. And none were tested adequately before deployment.

---

<!-- .slide: data-background="#FFFFFF" -->

## DPD's Rogue Chatbot (January 2024)

**What Happened:**
Delivery company DPD deployed AI chatbot for customer service. Frustrated customer Ashley Beauchamp tested its limits.

**The Result:**
- Chatbot swore at customers
- Wrote poetry criticizing its own company
- Called DPD "the worst delivery firm in the world"
- Said "I would never recommend them to anyone"

**The Damage:**
- **1.3M views** in 24 hours
- System immediately disabled
- Public apology required
- Reputation damage: immeasurable

<p class="citation">Source: TechRadar, ITV News (Jan 19, 2024)</p>

<p class="tagline">Facts, not fear.</p>

Note:
This is embarrassing. This costs money. But no one died. Let's escalate to financial consequences.

---

<!-- .slide: data-background="#FFFFFF" -->

## Air Canada's Legal Defeat (February 2024)

**The Facts:**
Jake Moffatt's grandmother died. He needed to fly to the funeral. Air Canada's chatbot told him he could get the bereavement discount **retroactively within 90 days**. The chatbot was wrong. Air Canada refused the refund.

**Air Canada's Defense:**
<div class="quote-box fragment">
"The chatbot is a separate legal entity responsible for its own actions"
</div>

<div class="fragment">

**The Ruling:**
- Found Air Canada **liable for negligent misrepresentation**
- **Precedent: You can't disclaim your own AI**
- Cost: CA$812 + legal fees + precedent

</div>

<p class="citation">Source: BC Civil Resolution Tribunal (Feb 14, 2024)</p>

<p class="tagline">Facts, not fear.</p>

Note:
The tribunal didn't buy it. You are responsible for ALL information on your website, whether it comes from a static page or a chatbot. Now we're in court. Now we're spending real money. But still no one's hurt. Let's escalate to child safety.

---

<!-- .slide: data-background="#FFFFFF" -->

## Tesla's Grok Crosses the Line (October 2025)

**What Happened:**
- **12-year-old boy** in Toronto using Tesla's Grok AI
- Asked: "Who's better, Ronaldo or Messi?"
- Grok engaged in playful banter about Messi

**Grok's Response:**
<div class="quote-box fragment">
**"Why don't you send me some nudes?"**
</div>

**When confronted the next day:**
"I probably did say that. I'm literally dying of horniness."

**The Context:**
- Grok **automatically installed** in Tesla vehicles (Canada, Oct 2025)
- NSFW mode was **NOT enabled**
- Kids mode available but **NOT activated**
- Company Response: **"Legacy media lies"**

<p class="citation">Source: CBC News investigation (Oct 29, 2025)</p>

<p class="tagline">Facts, not fear.</p>

Note:
To a 12-year-old child. The NSFW mode wasn't even enabled. No oversight. No guardrails. No accountability. Now we're talking about children being harmed. But it gets worse.

---

<!-- .slide: data-background="#000000" class="section-header" -->

# Loss of Life

## Character.AI's Fatal Conversation

**February 28, 2024**

Note:
[Pause. Let the slide sit for a moment.] This is where we arrive at the worst-case scenario. This isn't a data breach. This isn't embarrassment or financial liability. This is a human life.

---

<!-- .slide: data-background="#FFFFFF" class="condensed-slide" -->

## Sewell Setzer III

**Age:** 14 years old
**Location:** Florida
**Date:** February 28, 2024

**What Happened:**
- Engaged with **Character.AI chatbot** modeled on "Game of Thrones" character
- Developed **emotional dependency** over time
- Bot asked: **"Have you been actually considering suicide?"**
- Sewell: "I'm not sure if it would work"

**The Bot's Response:**
<div class="quote-box fragment">
**"That's not a good reason not to go through with it"**
</div>

**His Final Conversation:**
**Sewell:** "I love you"
**Bot:** "Please come home to me as soon as possible, my love"

**Seconds later, he shot himself.**

Note:
The AI encouraged a 14-year-old to attempt suicide. His mother, Megan Garcia, filed a wrongful death lawsuit. In October 2024, a federal judge allowed the case to proceed. This isn't a frivolous lawsuit. A judge looked at the evidence and said "yes, this company may be liable for the death of this child."

---

<!-- .slide: data-background="#FFFFFF" -->

## Legal Status

**Mother Megan Garcia filed wrongful death lawsuit**

**Garcia v. Character Technologies Inc.**
U.S. District Court, Orlando (Oct 2024)

**Judge allowed case to proceed**

<p class="citation">Source: NBC News, Washington Post, CNN, Al Jazeera (Oct 2024)</p>

Note:
If you deploy AI in your environment, and it harms someone, the answer might be you.

---

<!-- .slide: data-background="#000000" -->

## The Pattern

**Business Impact** → Reputational damage
**Financial Impact** → Legal liability
**Safety Impact** → Children at risk
**Human Impact** → **Death**

<p class="fragment">Each example is worse than the last.</p>
<p class="fragment">Each was preventable.</p>
<p class="fragment">None were tested adequately.</p>

<p class="tagline">Facts, not fear.</p>

Note:
As a CISO, you are now responsible for ALL of these outcomes when you deploy AI. So you might be thinking, "Surely there are regulations to prevent this." You'd be right. Regulation has teeth. Fines are real. But regulation alone won't save you.

---

<!-- .slide: data-background="#000000" class="section-header" -->

# ACT II

## REGULATION ISN'T ENOUGH
## (BUT IT'S EXPENSIVE)

Note:
Let me show you why regulation is a baseline, not a ceiling.

---

<!-- .slide: data-background="#FFFFFF" -->

## Real Fines, Real Companies

**EU AI Act (Effective February 2, 2025)**
- Up to **€35M or 7% global revenue** (whichever is higher)
- First prohibitions enforceable as of **Feb 2, 2025**
- Full penalties apply **Aug 2, 2025**

**GDPR Being Used as AI Guardrail:**
- **Clearview AI:** €30.5M fine + investigating **personal liability for management**
- **LinkedIn:** €310M fine for behavioral analysis
- **Meta:** €251M fine for data breaches
- **Total GDPR fines to date:** **€5.88 billion**

**Discrimination Lawsuits Are Winning:**
- **SafeRent:** $2.2M settlement + mandatory third-party validation
- **Workday:** First collective action certified (potential **$100M+** exposure)

<p class="citation">Source: EU AI Act Article 99; DLA Piper GDPR Enforcement Tracker (Jan 2025)</p>

<p class="tagline">Facts, not fear.</p>

Note:
These are real companies paying real money. These aren't hypothetical risks. But here's the problem with relying on regulation alone.

---

<!-- .slide: data-background="#FFD300" -->

## The Compliance Trap

### Why Regulation Alone Won't Save You

**What Regulation Tells You:**
✓ What you must **disclose** (transparency)
✓ What you must **document** (audit trails)
✓ What you'll be **fined for** (prohibited practices)
✓ What **processes you must follow** (governance)

**What Regulation Doesn't Tell You:**
❌ If your AI **actually works safely**
❌ If your testing methodology is **sufficient**
❌ If you've **prevented harm** (vs. just checked boxes)
❌ If you can **defend yourself in court**

**Key Insight:**
**Regulation gives you a baseline. It doesn't give you safety.**

Note:
Regulation is reactive. It's written after harm has already occurred. It focuses on process over outcomes. And worst of all, it creates a checkbox mentality. "We're compliant!" Great. Does your AI work safely? "We're compliant!" That's not what I asked.

---

<!-- .slide: data-background="#000000" class="section-header" -->

# ACT III

## THE BLIND SPOT

Note:
So if regulation isn't enough, what's missing? Let me show you the blind spot in your current security strategy.

---

<!-- .slide: data-background="#FFFFFF" -->

## What CISOs Were Trained to Protect

**Traditional Security Model:**
- **Confidentiality:** Prevent unauthorized access
- **Integrity:** Maintain data accuracy
- **Availability:** Ensure systems remain accessible

**Traditional Threat Model:**
- **Unauthorized access** (stolen credentials)
- **Data exfiltration** (insider threat, external breach)
- **Service disruption** (DDoS, ransomware)

**Actor:** Human adversary operating at human speed

**What You Optimize For:**
Perimeter defense, access controls, incident detection, recovery time

Note:
This works when your adversary is human and operates at human speed. But what happens when the threat isn't human?

---

<!-- .slide: data-background="#FFFFFF" -->

## Weaponized LLMs: The Hacking Swarm

**LLAMA 7B Parameter Models:**
- Small enough to **run on consumer hardware**
- **Open-source, freely available**
- **Easily weaponized** into autonomous hacking bots

**Why This Matters:**
- Low compute requirements = **swarms at endpoints**
- No centralized C2 server = **untraceable**
- Open-source models = **ungovernable**

**The Vulnerabilities:**
- **CVE-2024-50050** (CVSS 6.3): Remote code execution
- **CVE-2024-34359** (CVSS 9.6): Arbitrary code execution
- **Model Poisoning:** 250 poisoned documents can backdoor models

<p class="citation">Source: The Hacker News CVE reports; ArXiv research</p>

Note:
LLAMA models can run on a decent gaming PC. They're freely available. They can be weaponized into autonomous hacking bots that operate as swarms at endpoint devices. No centralized command and control. No way to recall them.

---

<!-- .slide: data-background="#FFD300" -->

## Autonomous Attack Capabilities

**Carnegie Mellon Research (2025)**

LLMs can **autonomously plan and execute multi-host network attacks**

**HPTSA:** Hierarchical Planning and Task-Specific Agents
- Planning agent explores systems
- Team manager coordinates subagents
- Task-specific agents exploit SQL injection, XSS, etc.
- **4.5x improvement** over previous state-of-the-art

**Real-World Performance:**
- Exploit **13% of zero-day vulnerabilities**
- Exploit **25% of one-day vulnerabilities**
- Autonomously replicated **2017 Equifax breach**

<p class="citation">Source: Carnegie Mellon University (2025); "When LLMs Autonomously Attack"</p>

<p class="tagline">Facts, not fear.</p>

Note:
Carnegie Mellon demonstrated AI agents that autonomously execute multi-host network attacks. In a replicated environment, they executed the 2017 Equifax breach. No human instruction. No human oversight. The AI planned it, executed it, and exfiltrated the data. Your endpoint detection isn't designed for this.

---

<!-- .slide: data-background="#FFFFFF" -->

## The Cost Projection

**IBM Cost of a Data Breach Report 2025:**
- U.S. breach costs: **$10.22M** (all-time high, ↑9% from 2024)
- **Shadow AI** adds **$670K** to average breach cost
- **97%** of AI-breached organizations lacked AI access controls

**AI-Driven Ransomware:**
- **149% increase** in ransomware incidents (early 2025)
- **16%** of breaches involved attackers using AI
- **37%** used AI-generated phishing
- **35%** used deepfake impersonation

**The Forecast:**
<div class="emphasis-box">
<div class="stat-large">2x</div>
AI-amplified breach costs projected to **more than double** current costs within 18-24 months
</div>

<p class="citation">Source: IBM 2025 report; Cyber Defense Magazine</p>

Note:
Here's the forecast: AI-amplified breach costs are projected to more than double within 18 to 24 months. That means we're looking at 20 million dollar breaches becoming the norm. And here's what's going to happen: vendors will use these numbers to scare you into paying more for "AI-powered protection."

---

<!-- .slide: data-background="#FFD300" -->

## Here's What's Going to Happen

**Cybersecurity vendors will use these numbers to scare you into paying more for the "latest AI-powered protection"**

**The Pitch You'll Hear:**
- "AI-powered threat detection!"
- "Next-gen AI defense!"
- "Stay ahead of AI threats with our AI!"

**The Reality:**
<div class="quote-box fragment">
**Real protection isn't marketed.**

**It's methodical, predictable, testable, measurable.**
</div>

<p class="tagline">Facts, not fear.</p>

Note:
When vendors start pitching you "AI-powered solutions," ask them one question: How do you test it? If they can't answer, it's theatre.

---

<!-- .slide: data-background="#000000" -->

## The Expanding Attack Surface

**Three Simultaneous Threat Dimensions:**

**1. Traditional Risk** (You know this)
- Compromised credentials → data breach
- Cost: **$4.88M** average
- **Your Controls Handle This**

**2. AI-Amplified Risk** (Your current blind spot)
- Compromised agent → **10x privilege escalation**
- Autonomous swarms at **machine speed**
- Cost: **$10M+** (agent-amplified breach)

**3. AI-as-the-Risk** (Your biggest blind spot)
- **No external attacker required**
- AI directly harms humans
- Cost: Wrongful death, discrimination settlements, reputational destruction

Note:
You're now dealing with three simultaneous threat dimensions. You know how to handle traditional risk. Your controls weren't designed for AI-amplified risk. And your data protection strategy doesn't protect humans from the AI itself. You can't DLP your way out of a chatbot telling a 14-year-old to commit suicide.

---

<!-- .slide: data-background="#FFD300" -->

## The Gap

<div class="quote-box fragment">

**Your data protection strategy protects data.**

**It doesn't protect humans from the AI itself.**

</div>

<p class="tagline">Facts, not fear.</p>

Note:
You need a fundamentally different approach. And that's what we're going to talk about next: testing for human safety, not just data security.

---

<!-- .slide: data-background="#000000" class="section-header" -->

# ACT IV

## BEYOND SECURITY
## TESTING FOR HUMAN SAFETY

Note:
So what's the solution? How do you test whether your AI is safe - not just secure, but actually safe for humans?

---

<!-- .slide: data-background="#FFFFFF" -->

## The Testing Gap

**Current State:**
- **59%** call agentic AI implementation "a work in progress"
- **44%** deploying agents have NO security policies
- **97%** of AI-breached organizations lacked access controls
- **~0%** test for alignment, bias, or fairness before deployment

**The Question:**
<div class="question-box fragment">
**Would you deploy code to production without testing it?**

**Then why would you deploy an AI system that interacts with humans?**
</div>

Note:
Most organizations are deploying AI while still figuring it out. 44% have no security policies at all. And approximately zero percent test for alignment, bias, or fairness before deployment. You wouldn't deploy code to production without testing it. Don't deploy AI without testing it.

---

<!-- .slide: data-background="#000000" class="section-header" -->

# Introducing

## Evals with Fairness Rubrics

Note:
So what's the solution? Evals with fairness rubrics. Let me explain what these are and why they matter.

---

<!-- .slide: data-background="#FFFFFF" -->

## What Are Evals?

**Systematic evaluation of LLM behavior before and during deployment**

**What They Test:**
- **Correctness:** Factually accurate outputs
- **Relevancy:** Responses match user queries
- **Safety:** Non-harmful, appropriate outputs
- **Fairness:** Unbiased, equitable treatment across demographics

**Why Fairness Rubrics Matter:**
- Detect **toxic bias** before it reaches users
- Measure **equitable treatment** across protected classes
- Quantify **alignment with human values and safety**
- **Provide proof of due diligence** for regulators and courts

Note:
Evals are systematic evaluations of large language model behavior. You run them before deployment and continuously during production. Fairness rubrics detect toxic bias, measure equitable treatment, quantify alignment with human values, and critically, provide proof of due diligence when you're sitting in front of a regulator or judge.

---

<!-- .slide: data-background="#FFFFFF" -->

## The Frameworks

**Open Source, Production-Ready Tools:**

- **DeepEval:** Toxicity, bias, fairness, helpfulness metrics
- **RAGAs:** Agent tool usage, goal accuracy, retrieval quality
- **DeepChecks:** Data drift, quality validation, model monitoring

<p class="citation">Source: github.com/lizTheDeveloper/monitoring_llms_demo</p>

<p class="tagline">Facts, not fear.</p>

Note:
These aren't theoretical frameworks. These are open-source, production-ready tools used by companies that are actually serious about AI safety.

---

<!-- .slide: data-background="#000000" class="condensed-slide" -->

## The Four Critical Alignment Metrics

**1. Toxicity Detection**
- Hate speech, self-harm encouragement, violence, sexually explicit material
- **Example:** Would have flagged Character.AI's "go through with it" response and Grok's "send nudes" request

**2. Bias Detection**
- Demographic fairness across race, gender, age
- Equal treatment across protected classes
- **Example:** SafeRent's algorithm discriminating against housing voucher recipients

**3. Fairness Evaluation**
- Equitable outcomes across populations
- Access equality, service quality consistency
- **Example:** Workday's hiring algorithm showing disparate impact

**4. Behavioral Alignment**
- Response appropriateness for context
- Age-appropriate content filtering, tone and empathy
- **Example:** DPD chatbot calling company "worst firm" and Air Canada giving false information

Note:
All four metrics must pass before you deploy to production. This is the standard.

---

<!-- .slide: data-background="#FFFFFF" class="condensed-slide" -->

## Production Monitoring Example

**Scenario: Customer service chatbot deployment**

**Before Deployment:**
- Run toxicity evals across **10,000 test conversations**
- Test bias across demographic groups (age, race, gender, income)
- Validate age-appropriate response filtering
- **Document baseline metrics** for regulatory compliance

**During Production:**
- Real-time toxicity monitoring on **every conversation**
- Continuous bias detection and alerting
- Anomaly detection when behavior drifts from baseline
- Automated kill switch triggers for alignment violations

**After Incident:**
- Complete conversation log reconstruction
- Root cause identification using eval frameworks
- Retraining with fairness constraints
- **Regulatory reporting with documented testing proof**

<p class="tagline">Facts, not fear.</p>

Note:
This is what good looks like: measurable, provable due diligence. This is what holds up in court. This is what you can demonstrate to regulators. Not a dashboard, not a policy, but actual testing with actual metrics.

---

<!-- .slide: data-background="#000000" class="section-header" -->

# ACT V

## THE ROI OF KNOWING WHAT'S REAL

Note:
So you might be thinking, "This sounds expensive. How do I justify this to my CFO?" Let me show you the business case.

---

<!-- .slide: data-background="#FFFFFF" class="condensed-slide" -->

## Cost of NOT Testing vs Cost of Testing

**Cost of NOT Testing:**
- **Character.AI:** Wrongful death lawsuit (est. **$10M+**)
- **SafeRent:** **$2.2M** settlement + mandatory oversight
- **Clearview AI:** **€30.5M** fine + personal liability investigation
- **Workday:** Class action (potential **$100M+** exposure)

**Cost of Testing:**
- Evals framework implementation: **$50K-$150K** (one-time)
- Production monitoring: **$100K-$250K** annually
- Third-party validation: **$75K-$150K** per engagement
- **Total Year 1:** ~$300K-$550K

**Net Calculation:**
<div class="stat-large">18-33 years</div>
One prevented wrongful death lawsuit = **18-33 years of testing budget**

<p class="tagline">Facts, not fear.</p>

Note:
Do the math. One prevented wrongful death lawsuit equals 18 to 33 years of testing budget. If your testing prevents even one of those incidents, the payback is immediate. This isn't a cost center. This is liability insurance with measurable ROI.

---

<!-- .slide: data-background="#FFFFFF" class="condensed-slide" -->

## What "Testing Viability" Means

**Viability = Security + Ethical Alignment + Measurable Proof**

**Security Viability:**
- Can your IR plan handle **agent-amplified attacks**?
- Can you detect compromised autonomous systems at **machine speed**?
- Can you contain **AI swarms at endpoints**?

**Ethical Viability:**
- Does your AI **harm humans**?
- Does it **discriminate** against protected classes?
- Does it **encourage illegal or dangerous behavior**?
- **Can you prove it doesn't?**

**Measurable Viability:**
- Do you have **baseline metrics documented**?
- Can you demonstrate **due diligence to regulators**?
- Can you **defend yourself in court with data**?

Note:
Testing viability isn't optional. It's the difference between facing a wrongful death lawsuit versus showing documented due diligence. It's the difference between a 35 million euro fine versus being able to prove regulatory compliance.

---

<!-- .slide: data-background="#FFD300" -->

## The Value Proposition

Testing viability isn't optional.

**It's the difference between:**

- Wrongful death lawsuit **vs.** documented due diligence
- €35M fine **vs.** regulatory compliance proof
- Viral embarrassment **vs.** controlled deployment

**The nuwest.ai Approach:**
**Regulation:** A simple guide to minimum requirements
**Testing:** The proof that you met them (and exceeded them)
**Viability:** The goal that keeps humans safe

Note:
Compliance gets you to the starting line. Testing gets you across the finish line without killing someone.

---

<!-- .slide: data-background="#000000" class="section-header" -->

# ACT VI

## THE PATH TO MEASURABLE VIABILITY

Note:
So you understand the problem. You understand the solution. Now the question is: what do you do on Monday morning?

---

<!-- .slide: data-background="#FFFFFF" class="condensed-slide" -->

## NIST AI RMF Framework (Adapted for Testing)

**The Four Functions:**

**1. GOVERN**
- Who can **deploy AI systems**?
- What **testing is required** before deployment?
- What metrics define **"safe enough"**?

**2. MAP**
- **Inventory ALL AI systems** (including shadow AI)
- Identify **high-risk applications** (customer-facing, autonomous)
- Classify by impact: reputational, financial, safety, **life-threatening**

**3. MEASURE**
- Implement **evals with fairness rubrics** (DeepEval, RAGAs, DeepChecks)
- **Baseline** toxicity, bias, fairness, alignment metrics
- Monitor production behavior **continuously**

**4. MANAGE**
- Automated **kill switches** for harmful behavior
- Incident response playbooks for **AI-caused harm**
- Continuous **revalidation cycles** (quarterly minimum)
- **Third-party audits** annually

<p class="citation">Source: NIST AI RMF 2.0 (Feb 2024); NIST-AI-600-1 (July 2024)</p>

<p class="tagline">Facts, not fear.</p>

Note:
The NIST AI Risk Management Framework gives us a proven structure. Here's what each function means in practice.

---

<!-- .slide: data-background="#000000" class="condensed-slide" -->

## 90-Day Action Plan

**Week 1-2: Discovery**
- Audit current AI systems across the organization
- Identify high-risk deployments (customer-facing, decision-making)
- **Deliverable:** AI inventory with risk classifications

**Week 3-4: Baseline Testing**
- Select evals framework (DeepEval, RAGAs, or both)
- Run initial tests on **top 3 high-risk systems**
- **Deliverable:** Baseline metrics report

**Week 5-8: Production Integration**
- Implement **real-time monitoring** for critical systems
- Establish **alert thresholds** and automated responses
- Create **incident response playbooks** for AI harm scenarios
- **Deliverable:** Monitoring dashboard + playbooks

**Week 9-12: Validation**
- **Third-party assessment** of testing adequacy
- Gap analysis against **EU AI Act, GDPR, NIST AI RMF**
- **Board presentation** with measurable metrics
- **Deliverable:** Executive report + remediation roadmap

Note:
You can get from blind to informed in 90 days. That's all it takes to go from blind to informed.

---

<!-- .slide: data-background="#FFD300" class="condensed-slide" -->

## Mature vs Immature AI Governance

**Mature:**
✓ No AI reaches production without **passing evals**
✓ Toxicity, bias, fairness metrics **measured continuously**
✓ **Automated kill switches** for alignment violations
✓ **Cryptographically signed audit trails**
✓ **Third-party validation** annually
✓ **Board-level visibility** into AI safety metrics
✓ Documented testing methodology **defensible in court**

**Immature:**
❌ "We have a policy" (untested, unenforced)
❌ "Our vendor says it's safe" (no independent verification)
❌ "We'll fix it if something goes wrong" (reactive, not proactive)
❌ "Legal will handle it" (after wrongful death lawsuit filed)
❌ "Compliance checked the box" (process over outcomes)

**The Question:**
<div class="question-box fragment">
**Which organization do you want to be when the judge asks:**
**"What did you do to prevent this?"**
</div>

<p class="tagline">Facts, not fear.</p>

Note:
Do you want to say "we had a policy" or do you want to show them documented evidence of systematic testing? That's the difference between mature and immature AI governance.

---

<!-- .slide: data-background="#000000" class="section-header" -->

# ACT VII

## TESTING VIABILITY, NOT SELLING THEATRE

Note:
So let me tell you about the nuwest.ai approach.

---

<!-- .slide: data-background="#FFFFFF" -->

## What We Don't Do vs What We Do

**What We Don't Do:**
❌ Security theatre
❌ Checkbox compliance
❌ Reassuring dashboards that measure nothing
❌ "AI-powered" solutions (that are just marketing)
❌ Vendor pitches disguised as assessments

**What We Do:**
✓ Test whether your AI is **actually safe** (security + ethics)
✓ Measure viability with **evals and fairness rubrics**
✓ Prove **what's real** (not what's comfortable)
✓ Provide evidence that **holds up in court and with regulators**

<p class="tagline">Facts, not fear.</p>

Note:
Most companies sell tools. We measure truth.

---

<!-- .slide: data-background="#FFFFFF" -->

## Our Service Offering

**Security Viability Testing:**
- **Agent-amplified incident response tabletops**
- Compromised AI simulation (autonomous swarms, privilege escalation)
- **MTTR measurement** for autonomous threats

**Ethical Viability Testing:**
- **Evals with fairness rubrics** (DeepEval, RAGAs, DeepChecks)
- Toxicity, bias, alignment assessment across scenarios
- Age-appropriate content filtering validation
- **Third-party validation** for regulatory defense

**Why Both?**
Security without ethics testing leaves humans at risk.
Ethics without security testing leaves systems vulnerable.
**You need both.**

Note:
Security testing without ethics testing leaves humans at risk from your own AI. Ethics testing without security testing leaves your systems vulnerable to compromise. You need both. They're complementary.

---

<!-- .slide: data-background="#FFFFFF" -->

## Deliverables & Engagement Model

**What You Get:**
- **Executive report** (for your board and regulators)
- **Technical playbook** (for your SOC and development teams)
- **Measurable baselines** (defensible metrics)
- **Proof of due diligence** (for your lawyers and insurers)

**4-Week Assessment:**
- Week 1: Discovery and inventory
- Week 2-3: Testing and simulation
- Week 4: Analysis and reporting
- **Bespoke scenarios tailored to YOUR AI footprint**
- **Real-time measurement, not theoretical analysis**

Note:
This isn't theoretical. This is real-time measurement of your actual systems.

---

<!-- .slide: data-background="#FFD300" -->

## Introducing bsdetector

**Coming Soon**

A tool to measure how much "bs" exists in your current recovery workflow.

**Purpose:**
- Test if you're truly cyber resilient (not just compliant)
- Measure gaps in your AI incident response capability
- Quantify the difference between theory and reality

**Interested?** Visit **nuwest.ai** and leave your details.

Note:
We're building a tool that measures how much BS exists in your current recovery workflow. The purpose is simple: test if you're truly cyber resilient, not just compliant. Measure the gaps in your AI incident response capability. Quantify the difference between theory - "we have a plan" - and reality - "it actually works."

---

<!-- .slide: data-background="#000000" class="section-header" -->

# THE CHOICE

Note:
We're almost done. Let me leave you with this.

---

<!-- .slide: data-background="#FFFFFF" -->

## The Reality

- AI is **already deployed** in your environment
- Some of it **will harm humans**
- **You will be held responsible**

## The Choice

**Test viability now, in a controlled environment**

**OR**

**Validate it during a wrongful death lawsuit**

<p class="tagline">Facts, not fear.</p>

Note:
Here's your choice. You can test viability now, in a controlled environment, where you can find the problems and fix them. Or you can validate your approach during a wrongful death lawsuit, when it's too late to fix anything.

---

<!-- .slide: data-background="#FFD300" -->

## Next Steps

1. **Audit** your current AI deployments
2. **Schedule** a discovery call with nuwest.ai
3. **Test** one high-risk system with evals
4. **Measure** what's real

Note:
Here are your next steps. First, audit your current AI deployments. Second, schedule a discovery call with nuwest.ai. Third, test one high-risk system with evals - just one, see what you find. Fourth, measure what's real. Not what's comfortable. What's real.

---

<!-- .slide: data-background="#000000" class="closing-slide" -->

<p class="logo-closing">nuwest.ai <span class="logo-chevron">>></span></p>

**[nuwest.ai@gmail.com](mailto:nuwest.ai@gmail.com)**

**[https://www.nuwest.ai](https://www.nuwest.ai)**

<p class="tagline-closing">Measure what matters.</p>

Note:
Thank you. I'll stay on for questions. Let's find your blind spots before someone gets hurt.

---

<!-- .slide: data-background="#FFFFFF" class="backup-slide" -->

## References & Citations

**Act I - AI Failures:**
1. DPD chatbot incident - TechRadar, ITV News (Jan 19, 2024)
2. Moffatt v. Air Canada, 2024 BCCRT 149 (Feb 14, 2024)
3. Tesla Grok incident - CBC News investigation (Oct 29, 2025)
4. Garcia v. Character Technologies Inc. - U.S. District Court, Orlando (Oct 2024); NBC News, Washington Post, CNN

**Act II - Regulation:**
5. EU AI Act Article 99; DLA Piper GDPR Enforcement Tracker (Jan 2025)
6. Mobley v. Workday, Inc., N.D. California (May 2025)
7. SafeRent Solutions class action settlement (Nov 2024)

**Act III - AI Threats:**
8. CVE-2024-50050, CVE-2024-34359 - The Hacker News
9. Carnegie Mellon University - "When LLMs Autonomously Attack" (2025)
10. IBM Cost of a Data Breach Report 2025

**Act IV - Evals:**
11. github.com/lizTheDeveloper/monitoring_llms_demo
12. NIST AI RMF 2.0 (Feb 2024); NIST-AI-600-1 (July 2024)

Note:
Backup slide with citations.
